# ===== ULTRAFOREXAI V2 - PARTE 1: CONFIGURACI√ìN INICIAL =====
print("üöÄ ULTRAFOREXAI V2: Sistema Multi-Timeframe con 4 Tipos de Trading")
print("üìä Parte 1: Configuraci√≥n inicial y setup")
print("üéØ Objetivo: 85%+ Scalping | 75%+ Day | 70%+ Swing | 65%+ Position")
print("=" * 80)

import os
import json
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from datetime import datetime, timedelta
import threading
import time
import logging
from pathlib import Path
import warnings
import random
from functools import wraps
from sklearn.metrics import accuracy_score
import pickle
warnings.filterwarnings('ignore')

# ===== PARCHE YAHOO FINANCE - RATE LIMITING Y MANEJO ROBUSTO =====

def rate_limit_yfinance(calls_per_minute=5):
    """Rate limiter para evitar bloqueos de Yahoo Finance"""
    def decorator(func):
        last_called = [0.0]
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = 60.0 / calls_per_minute - elapsed
            if left_to_wait > 0:
                logger.info(f"‚è∏Ô∏è Esperando {left_to_wait:.1f} segundos...")
                time.sleep(left_to_wait)
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        return wrapper
    return decorator

@rate_limit_yfinance(calls_per_minute=4)  # Muy conservador
def fixed_yf_download(symbol, period='1mo', interval='1d', max_retries=5):
    """
    Reemplazo robusto para yf.download con manejo de errores mejorado
    """
    
    # Mapeo de s√≠mbolos problem√°ticos a alternativas
    symbol_alternatives = {
        'EURUSD=X': ['EURUSD=X', 'EUR=X', 'EURUSD.FOREX'],
        'USDJPY=X': ['USDJPY=X', 'JPY=X', 'USDJPY.FOREX'], 
        'GBPUSD=X': ['GBPUSD=X', 'GBP=X', 'GBPUSD.FOREX'],
        'AUDUSD=X': ['AUDUSD=X', 'AUD=X', 'AUDUSD.FOREX'],
        'USDCAD=X': ['USDCAD=X', 'CAD=X', 'USDCAD.FOREX']
    }
    
    symbols_to_try = symbol_alternatives.get(symbol, [symbol])
    
    for symbol_variant in symbols_to_try:
        for attempt in range(max_retries):
            try:
                logger.info(f"üîÑ Intento {attempt + 1}: {symbol_variant}")
                
                # Usar diferentes m√©todos seg√∫n el intento
                if attempt == 0:
                    # M√©todo 1: Ticker individual
                    ticker = yf.Ticker(symbol_variant)
                    data = ticker.history(period=period, interval=interval, auto_adjust=True)
                elif attempt == 1:
                    # M√©todo 2: Download directo
                    data = yf.download(symbol_variant, period=period, interval=interval, 
                                     progress=False, show_errors=False)
                elif attempt == 2:
                    # M√©todo 3: Con threads=False
                    data = yf.download(symbol_variant, period=period, interval=interval,
                                     progress=False, show_errors=False, threads=False)
                else:
                    # M√©todo 4: Per√≠odo alternativo
                    alt_periods = ['1mo', '3mo', '6mo']
                    alt_period = alt_periods[attempt - 3] if attempt - 3 < len(alt_periods) else '1mo'
                    data = yf.download(symbol_variant, period=alt_period, interval=interval,
                                     progress=False, show_errors=False, threads=False)
                
                # Validar datos
                if not data.empty and len(data) >= 10:
                    logger.info(f"‚úÖ √âxito: {symbol_variant} -> {len(data)} registros")
                    return data
                else:
                    logger.warning(f"‚ö†Ô∏è Datos insuficientes: {len(data) if not data.empty else 0} registros")
                    
            except Exception as e:
                logger.warning(f"‚ùå Error intento {attempt + 1}: {e}")
                
            # Pausa incremental entre intentos
            time.sleep((attempt + 1) * 2)
    
    # Si todo falla, devolver DataFrame vac√≠o en lugar de crash
    logger.error(f"‚ùå FALLO TOTAL para {symbol}")
    return pd.DataFrame()

def get_market_data_fixed(symbol, period, trading_style):
    """
    Reemplazo de la funci√≥n get_market_data actual con manejo robusto
    """
    # Mapear estilos a per√≠odos m√°s generosos
    period_mapping = {
        'scalping': '4mo',      # AUMENTADO de 1mo a 4mo para m√°s datos
        'day_trading': '6mo',    # AUMENTADO de 3mo a 6mo
        'swing_trading': '1y',  # Mantener
        'position_trading': '5y' # Mantener
    }
    
    actual_period = period_mapping.get(trading_style, period)
    
    logger.info(f"üìä Obteniendo datos: {symbol} ({trading_style} -> {actual_period})")
    
    # Usar la funci√≥n fija
    data = fixed_yf_download(symbol, period=actual_period)
    
    if data.empty:
        raise Exception("No se pudieron obtener datos despu√©s de varios intentos")
    
    if len(data) < 10:
        raise Exception(f"Datos insuficientes: solo {len(data)} registros")
    
    return data

# Configurar logging avanzado
os.makedirs('logs_v2', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs_v2/ultraforexai_v2.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class UltraForexAI_V2:
    """Sistema Ultra Completo Multi-Timeframe con 4 Tipos de Trading"""
    
    def __init__(self, models_dir="models_v2", logs_dir="logs_v2"):
        self.models_dir = Path(models_dir)
        self.logs_dir = Path(logs_dir)
        self.models_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)
        
        # Pares FOREX
        self.forex_pairs = {
            'EURUSD': 'EURUSD=X',
            'USDJPY': 'USDJPY=X', 
            'GBPUSD': 'GBPUSD=X',
            'AUDUSD': 'AUDUSD=X',
            'USDCAD': 'USDCAD=X'
        }
        
        # Configuraci√≥n del sistema mejorada
        self.system_config = {
            'auto_retrain_hours': 24,
            'hyperopt_trials': 50,  # Reducido para velocidad
            'ensemble_threshold': 0.70,
            'max_features': 15,
            'validation_split': 0.15
        }
        
        # Configurar estilos de trading
        self.setup_trading_styles()
        
        # Resultados del sistema
        self.system_results = {
            'models_trained': {},
            'style_performance': {},
            'hyperopt_results': {},
            'ensemble_performance': {},
            'auto_training_log': [],
            'started_at': datetime.now().isoformat()
        }
        
        # Thread de auto-entrenamiento
        self.auto_training_active = False
        self.auto_training_thread = None
        
        logger.info("‚úÖ UltraForexAI V2 inicializado correctamente")
    
    def setup_trading_styles(self):
        """Configuraci√≥n espec√≠fica para cada estilo de trading"""
        
        self.trading_styles = {
            'scalping': {
                'timeframe': '5min',
                'data_period': '4mo',  # AUMENTADO para m√°s datos
                'horizon_minutes': [5, 15, 30],
                'target_precision': 0.85,  # Target original del script exitoso
                'max_signals_per_hour': 12,
                'min_move_threshold': 0.0004,  # 4 pips
                'spread_cost': 0.0003,  # 3 pips spread
                'session_weights': {
                    'overlap': 2.0,
                    'london': 1.5,
                    'ny': 1.5, 
                    'asian': 0.2
                },
                'features_priority': [
                    'order_flow', 'microstructure', 'volume_profile',
                    'bid_ask_spread', 'momentum_short', 'session_timing'
                ]
            },
            
            'day_trading': {
                'timeframe': '15min',
                'data_period': '3mo',  # Conservador como en el script exitoso
                'horizon_minutes': [60, 120, 240, 480],
                'target_precision': 0.75,  # Target original del script exitoso
                'max_signals_per_day': 20,
                'min_move_threshold': 0.0008,  # 8 pips
                'spread_cost': 0.0003,
                'session_weights': {
                    'overlap': 1.8,
                    'london': 1.4,
                    'ny': 1.4,
                    'asian': 0.6
                },
                'features_priority': [
                    'momentum_multi', 'volatility_regime', 'support_resistance',
                    'session_patterns', 'volume_analysis', 'rsi_divergence'
                ]
            },
            
            'swing_trading': {
                'timeframe': '1h',
                'data_period': '1y',
                'horizon_hours': [24, 48, 72, 120],
                'target_precision': 0.70,  # Target original del script exitoso
                'max_signals_per_week': 50,
                'min_move_threshold': 0.0020,  # 20 pips
                'spread_cost': 0.0003,
                'trend_focus': True,
                'features_priority': [
                    'trend_strength', 'breakout_patterns', 'fibonacci_levels',
                    'weekly_patterns', 'momentum_medium', 'correlation_analysis'
                ]
            },
            
            'position_trading': {
                'timeframe': '1D',
                'data_period': '2y',  # Mantener como en el script exitoso
                'horizon_days': [7, 14, 21],  # Mantener horizontes reducidos
                'target_precision': 0.65,  # Target original del script exitoso
                'max_signals_per_month': 20,
                'min_move_threshold': 0.0050,  # Mantener 50 pips
                'spread_cost': 0.0003,
                'fundamental_weight': 0.4,
                'features_priority': [
                    'macro_trends', 'long_term_momentum', 'seasonal_patterns',
                    'monthly_patterns', 'correlation_majors', 'economic_cycles'
                ]
            }
        }
        
        logger.info("‚úÖ Trading styles configurados correctamente")

    def get_style_config(self, trading_style):
        """Obtener configuraci√≥n para estilo espec√≠fico"""
        if trading_style not in self.trading_styles:
            raise ValueError(f"Estilo {trading_style} no soportado. Usar: {list(self.trading_styles.keys())}")
        
        return self.trading_styles[trading_style]

    def install_missing_dependencies(self):
        """Instalar dependencias faltantes para el sistema V2"""
        logger.info("üì¶ Verificando dependencias V2...")
        dependencies = {
            'optuna': 'optuna',
            'xgboost': 'xgboost', 
            'lightgbm': 'lightgbm',
            'ta': 'ta',
            'yfinance': 'yfinance'
        }
        missing = []
        for module, package in dependencies.items():
            try:
                __import__(module)
                logger.info(f"‚úÖ {module}")
            except ImportError:
                missing.append(package)
                logger.info(f"‚ùå {module}")
        if missing:
            logger.info(f"üì¶ Instalando: {missing}")
            for package in missing:
                os.system(f"pip install {package}")
            logger.info("‚úÖ Dependencias V2 instaladas")
        
        # Verificar TensorFlow
        try:
            tf.config.list_physical_devices('GPU')
            logger.info("‚úÖ TensorFlow configurado")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TensorFlow warning: {e}")

    def clean_old_models(self):
        """Limpiar modelos antiguos y verificar directorio"""
        try:
            logger.info("üßπ Limpiando modelos antiguos...")
            if not self.models_dir.exists():
                self.models_dir.mkdir(exist_ok=True)
                logger.info(f"‚úÖ Directorio creado: {self.models_dir}")
            model_files = list(self.models_dir.rglob("*.pkl"))
            logger.info(f"üìÅ Modelos encontrados: {len(model_files)}")
            for model_file in model_files:
                logger.info(f"   üìÑ {model_file}")
            for symbol in self.forex_pairs.values():
                symbol_dir = self.models_dir / symbol
                if symbol_dir.exists():
                    symbol_files = list(symbol_dir.glob("*.pkl"))
                    logger.info(f"   üìÇ {symbol}: {len(symbol_files)} modelos")
                    for file in symbol_files:
                        logger.info(f"      üìÑ {file.name}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error limpiando modelos: {e}")
            return False

    def verify_model_structure(self):
        """Verificar estructura de directorios de modelos"""
        try:
            logger.info("üîç Verificando estructura de modelos...")
            if not self.models_dir.exists():
                self.models_dir.mkdir(exist_ok=True)
                logger.info(f"‚úÖ Directorio principal creado: {self.models_dir}")
            for symbol in self.forex_pairs.values():
                symbol_dir = self.models_dir / symbol
                if not symbol_dir.exists():
                    symbol_dir.mkdir(exist_ok=True)
                    logger.info(f"‚úÖ Directorio creado para {symbol}: {symbol_dir}")
            test_file = self.models_dir / "test_write.tmp"
            try:
                with open(test_file, 'w') as f:
                    f.write("test")
                test_file.unlink()  # Eliminar archivo de prueba
                logger.info("‚úÖ Permisos de escritura verificados")
            except Exception as e:
                logger.error(f"‚ùå Error de permisos: {e}")
                return False
            logger.info("‚úÖ Estructura de modelos verificada")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error verificando estructura: {e}")
            return False

print("‚úÖ Parte 1 completada - Configuraci√≥n inicial lista")
print("üìù Variables creadas:")
print("   - Clase UltraForexAI_V2")
print("   - Trading styles configurados")
print("   - Logging configurado")
print("   - Dependencias verificadas")

# ===== ULTRAFOREXAI V2 - PARTE 2: OBTENCI√ìN DE DATOS =====
print("üìä PARTE 2: Sistema de obtenci√≥n de datos multi-timeframe")

# AGREGAR ESTOS M√âTODOS A LA CLASE UltraForexAI_V2:

def get_enhanced_data_multi_fixed(self, symbol, trading_style, period=None):
    """
    VERSI√ìN CORREGIDA - Obtener datos optimizados usando el fix de Yahoo Finance
    Reemplaza el m√©todo original get_enhanced_data_multi
    """
    
    def add_time_features_safe(data):
        """Agregar features de tiempo de forma segura y robusta"""
        try:
            # Buscar columna de fecha
            date_col = None
            for col in ['Date', 'Datetime', 'date', 'datetime']:
                if col in data.columns:
                    date_col = col
                    break
            
            if date_col is not None:
                # Convertir a datetime si no lo est√°
                if not pd.api.types.is_datetime64_any_dtype(data[date_col]):
                    data[date_col] = pd.to_datetime(data[date_col])
                
                # Features temporales m√°s robustos
                data['Hour'] = data[date_col].dt.hour
                data['DayOfWeek'] = data[date_col].dt.dayofweek
                data['Month'] = data[date_col].dt.month
                data['Quarter'] = data[date_col].dt.quarter
                data['IsWeekend'] = (data['DayOfWeek'] >= 5).astype(int)
                
                # Features de sesiones de trading
                data['LondonSession'] = ((data['Hour'] >= 8) & (data['Hour'] <= 16)).astype(int)
                data['NYSession'] = ((data['Hour'] >= 13) & (data['Hour'] <= 21)).astype(int)
                data['AsianSession'] = ((data['Hour'] >= 0) & (data['Hour'] <= 8)).astype(int)
                data['OverlapSession'] = (data['LondonSession'] & data['NYSession']).astype(int)
                
                # Features de volatilidad temporal
                data['MondayEffect'] = (data['DayOfWeek'] == 0).astype(int)
                data['FridayEffect'] = (data['DayOfWeek'] == 4).astype(int)
                data['MonthEnd'] = (data[date_col].dt.day >= 25).astype(int)
                
            else:
                # Fallback con datos sint√©ticos para evitar varianza cero
                np.random.seed(42)  # Para reproducibilidad
                data['Hour'] = np.random.randint(0, 24, len(data))
                data['DayOfWeek'] = np.random.randint(0, 7, len(data))
                data['Month'] = np.random.randint(1, 13, len(data))
                data['Quarter'] = np.random.randint(1, 5, len(data))
                data['IsWeekend'] = np.random.choice([0, 1], len(data))
                data['LondonSession'] = np.random.choice([0, 1], len(data))
                data['NYSession'] = np.random.choice([0, 1], len(data))
                data['AsianSession'] = np.random.choice([0, 1], len(data))
                data['OverlapSession'] = np.random.choice([0, 1], len(data))
                data['MondayEffect'] = np.random.choice([0, 1], len(data))
                data['FridayEffect'] = np.random.choice([0, 1], len(data))
                data['MonthEnd'] = np.random.choice([0, 1], len(data))
            
            return data
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error agregando features de tiempo: {e}")
            # Fallback b√°sico con variabilidad
            np.random.seed(42)
            data['Hour'] = np.random.randint(0, 24, len(data))
            data['DayOfWeek'] = np.random.randint(0, 7, len(data))
            data['Month'] = np.random.randint(1, 13, len(data))
            data['IsWeekend'] = np.random.choice([0, 1], len(data))
            return data
    
    style_config = self.get_style_config(trading_style)
    
    # Usar per√≠odo de configuraci√≥n o el proporcionado
    data_period = period or style_config['data_period']
    
    logger.info(f"üìä Obteniendo datos CORREGIDOS para {trading_style} - {symbol}")
    
    try:
        import yfinance as yf
        import ta
        
        # USAR EL FIX QUE YA FUNCIONA
        symbol_alternatives = {
            'EURUSD=X': ['EURUSD=X', 'EUR=X', 'EURUSD.FOREX'],
            'USDJPY=X': ['USDJPY=X', 'JPY=X', 'USDJPY.FOREX'],
            'GBPUSD=X': ['GBPUSD=X', 'GBP=X', 'GBPUSD.FOREX'],
            'AUDUSD=X': ['AUDUSD=X', 'AUD=X', 'AUDUSD.FOREX'],
            'USDCAD=X': ['USDCAD=X', 'CAD=X', 'USDCAD.FOREX']
        }

        # Per√≠odos m√°s conservadores como en el script exitoso
        period_mapping = {
            'scalping': '4mo',      # Conservador como en el exitoso
            'day_trading': '6mo',   # Conservador como en el exitoso
            'swing_trading': '1y',  # Mantener
            'position_trading': '5y' # Mantener
        }
        
        actual_period = period_mapping.get(trading_style, data_period)
        symbols_to_try = symbol_alternatives.get(symbol, [symbol])
        
        logger.info(f"üîÑ Probando {len(symbols_to_try)} variantes de s√≠mbolo para {symbol}")
        
        # Intentar con cada s√≠mbolo alternativo
        data = None
        for sym_variant in symbols_to_try:
            try:
                logger.info(f"üîÑ Intentando {sym_variant} con per√≠odo {actual_period}...")
                
                # M√©todo que sabemos que funciona
                ticker = yf.Ticker(sym_variant)
                data = ticker.history(period=actual_period, auto_adjust=True)
                
                if not data.empty and len(data) >= 20:  # Conservador para estabilidad
                    logger.info(f"‚úÖ {sym_variant}: {len(data)} registros obtenidos")
                    break
                else:
                    logger.warning(f"‚ö†Ô∏è {sym_variant}: Datos insuficientes ({len(data) if not data.empty else 0})")
                    
            except Exception as e:
                logger.warning(f"‚ùå {sym_variant}: {str(e)[:50]}...")
                
            # Pausa entre intentos
            time.sleep(2)
        
        if data is None or data.empty:
            raise Exception("No se pudieron obtener datos con ninguna variante de s√≠mbolo")
        
        data.reset_index(inplace=True)
        
        # Resetear index y preparar datos
        data = data.reset_index()
        
        # Verificar columnas requeridas
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise Exception(f"Columnas faltantes: {missing_columns}")
        
        # Limpiar datos b√°sicos
        data = data.dropna(subset=required_columns)
        
        # AGREGAR FEATURES DE TIEMPO PRIMERO (ANTES de features espec√≠ficos)
        data = add_time_features_safe(data)
        
        # Agregar features espec√≠ficos por estilo (usar m√©todos existentes)
        try:
            if trading_style == 'scalping':
                data = self.add_scalping_features(data)
            elif trading_style == 'day_trading':
                data = self.add_day_trading_features(data)
            elif trading_style == 'swing_trading':
                data = self.add_swing_trading_features(data)
            elif trading_style == 'position_trading':
                data = self.add_position_trading_features(data)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error agregando features {trading_style}: {e}")
            # Continuar con datos b√°sicos
        
        # Target espec√≠fico por estilo (usar m√©todo existente)
        try:
            data['Direction'] = self.create_style_target(data, trading_style)
            
            # Verificar que el target tiene m√∫ltiples clases
            unique_classes = data['Direction'].unique()
            if len(unique_classes) < 2:
                logger.warning(f"‚ö†Ô∏è Target con una sola clase: {unique_classes}")
                # Crear target b√°sico como fallback
                basic_return = data['Close'].pct_change(5)
                data['Direction'] = np.where(basic_return > 0.001, 2, 
                                           np.where(basic_return < -0.001, 0, 1))
                logger.info(f"üîÑ Target fallback creado con clases: {data['Direction'].unique()}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error creando target {trading_style}: {e}")
            # Target b√°sico como fallback
            basic_return = data['Close'].pct_change(5)
            data['Direction'] = np.where(basic_return > 0.001, 2, 
                                       np.where(basic_return < -0.001, 0, 1))
            logger.info(f"üîÑ Target fallback creado despu√©s de error")
        
        # REMOVER LA L√çNEA DUPLICADA - Ya se ejecut√≥ arriba
        # data = add_time_features_safe(data)
        
        # Limpiar datos finales
        # Remover infinitos y NaN
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        data[numeric_columns] = data[numeric_columns].replace([np.inf, -np.inf], np.nan)
        
        # Forward fill y backward fill para datos faltantes
        data[numeric_columns] = data[numeric_columns].fillna(method='ffill').fillna(method='bfill')
        
        # Remover features con varianza cero o muy baja
        for col in numeric_columns:
            if col in data.columns:
                variance = data[col].var()
                if variance < 1e-10 or pd.isna(variance):
                    # Para features de tiempo, usar valores por defecto
                    if col in ['Hour', 'DayOfWeek', 'Month', 'IsWeekend']:
                        if col == 'Hour':
                            data[col] = 12  # Hora por defecto
                        elif col == 'DayOfWeek':
                            data[col] = 1   # Lunes por defecto
                        elif col == 'Month':
                            data[col] = 1   # Enero por defecto
                        elif col == 'IsWeekend':
                            data[col] = 0   # No es fin de semana
                    else:
                        data[col] = 0  # Reemplazar con 0 para otros features
        
        # Remover filas que a√∫n tengan NaN
        cleaned_data = data.dropna()
        
        if len(cleaned_data) < 5:  # M√ÅS PERMISIVO para scalping
            raise Exception(f"Datos insuficientes despu√©s de limpieza: {len(cleaned_data)} registros")
        
        logger.info(f"‚úÖ {symbol} {trading_style}: {len(cleaned_data)} registros limpios obtenidos")
        
        return cleaned_data
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo datos CORREGIDOS {trading_style} para {symbol}: {e}")
        return None

# ===== REEMPLAZAR M√âTODO ORIGINAL =====
# El m√©todo ya fue reemplazado arriba en la secci√≥n de asignaci√≥n de m√©todos

def resample_to_4h(self, data):
    """Resample datos de 1h a 4h"""
    try:
        data_copy = data.copy()
        data_copy.set_index('Datetime', inplace=True)
        
        # Resample con agregaci√≥n correcta
        resampled = data_copy.resample('4H').agg({
            'Open': 'first',
            'High': 'max',
            'Low': 'min',
            'Close': 'last',
            'Volume': 'sum'
        }).dropna()
        
        resampled.reset_index(inplace=True)
        
        logger.info(f"üìä Resample 1h ‚Üí 4h: {len(data)} ‚Üí {len(resampled)} registros")
        
        return resampled
        
    except Exception as e:
        logger.error(f"‚ùå Error en resample: {e}")
        return data

def validate_data_quality(self, data, trading_style):
    """Validar calidad de datos antes del entrenamiento"""
    
    issues = []
    
    # 1. Verificar tama√±o m√≠nimo (m√°s permisivo para scalping)
    min_samples = {
        'scalping': 50,    # REDUCIDO para permitir scalping
        'day_trading': 60,  # REDUCIDO para day trading
        'swing_trading': 40, # REDUCIDO para swing
        'position_trading': 25 # REDUCIDO para position
    }
    
    if len(data) < min_samples.get(trading_style, 500):
        issues.append(f"Datos insuficientes: {len(data)} < {min_samples[trading_style]}")
    
    # 2. Verificar distribuci√≥n del target
    if 'Direction' in data.columns:
        target_dist = data['Direction'].value_counts()
        if len(target_dist) < 2:
            issues.append("Target tiene una sola clase")
        
        # Verificar balance m√≠nimo
        min_class_ratio = 0.1
        class_ratios = target_dist / len(data)
        if class_ratios.min() < min_class_ratio:
            issues.append(f"Desbalance extremo en target: {class_ratios.to_dict()}")
    
    # 3. Verificar variabilidad de features (m√°s permisivo)
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    low_variance_cols = []
    
    # Features que pueden tener varianza baja sin ser problem√°ticos
    acceptable_low_variance = [
        'Volume', 'Dividends', 'Stock Splits',  # No aplican para FOREX
        'Hour', 'DayOfWeek', 'Month', 'IsWeekend',  # Features temporales
        'LondonSession', 'NYSession', 'AsianSession', 'OverlapSession',  # Sesiones
        'MondayEffect', 'FridayEffect', 'MonthEnd'  # Efectos temporales
    ]
    
    for col in numeric_cols:
        if col in ['Direction'] or col in acceptable_low_variance:
            continue
        try:
            variance = data[col].var()
            if variance < 1e-10 or pd.isna(variance):
                low_variance_cols.append(col)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error verificando varianza de {col}: {e}")
            low_variance_cols.append(col)
    
    # Solo reportar si hay muchos features con varianza cero
    if len(low_variance_cols) > 5:  # M√°s permisivo
        issues.append(f"Features con varianza cero: {low_variance_cols}")
    elif low_variance_cols:
        logger.info(f"‚ÑπÔ∏è Algunos features con varianza baja (aceptable): {low_variance_cols}")
    
    # 4. Verificar outliers extremos
    extreme_outliers = []
    for col in numeric_cols:
        if col in ['Direction', 'Hour', 'DayOfWeek', 'Month']:
            continue
        
        q99 = data[col].quantile(0.99)
        q01 = data[col].quantile(0.01)
        iqr = q99 - q01
        
        if iqr > 0:
            outlier_ratio = ((data[col] > q99 + 5*iqr) | (data[col] < q01 - 5*iqr)).mean()
            if outlier_ratio > 0.05:  # M√°s del 5% outliers extremos
                extreme_outliers.append(col)
    
    if extreme_outliers:
        issues.append(f"Features con outliers extremos: {extreme_outliers}")
    
    # Log resultados
    if issues:
        logger.warning(f"‚ö†Ô∏è Problemas de calidad en datos {trading_style}:")
        for issue in issues:
            logger.warning(f"   - {issue}")
    else:
        logger.info(f"‚úÖ Calidad de datos {trading_style} verificada")
    
    return len(issues) == 0, issues

# AGREGAR ESTOS M√âTODOS A LA CLASE (contin√∫a en el constructor)
UltraForexAI_V2.get_enhanced_data_multi = get_enhanced_data_multi_fixed
UltraForexAI_V2.resample_to_4h = resample_to_4h
UltraForexAI_V2.validate_data_quality = validate_data_quality

def validate_market_data_realism(self, data, symbol):
    """Validar que los datos de mercado sean realistas"""
    
    issues = []
    
    try:
        # 1. Verificar rangos de precios realistas
        price_changes = data['Close'].pct_change().abs()
        extreme_changes = price_changes > 0.05  # M√°s del 5% en un per√≠odo
        
        if extreme_changes.sum() > len(data) * 0.01:  # M√°s del 1% de datos extremos
            issues.append(f"Demasiados cambios de precio extremos: {extreme_changes.sum()}")
        
        # 2. Verificar volatilidad realista
        volatility = data['Close'].pct_change().rolling(20).std()
        avg_volatility = volatility.mean()
        
        # Volatilidad t√≠pica FOREX: 0.5% - 2% diario
        if avg_volatility < 0.001 or avg_volatility > 0.03:
            issues.append(f"Volatilidad no realista: {avg_volatility:.4f}")
        
        # 3. Verificar spreads realistas
        if 'bid_ask_spread' in data.columns:
            avg_spread = data['bid_ask_spread'].mean()
            if avg_spread > 0.001:  # M√°s de 10 pips promedio
                issues.append(f"Spread promedio muy alto: {avg_spread:.6f}")
        
        # 4. Verificar volumen realista
        if 'Volume' in data.columns:
            zero_volume = (data['Volume'] == 0).sum()
            if zero_volume > len(data) * 0.5:  # M√°s del 50% sin volumen
                issues.append(f"Muchos per√≠odos sin volumen: {zero_volume}")
        
        # 5. Verificar gaps realistas
        gaps = np.abs(data['Open'] - data['Close'].shift(1)) / data['Close'].shift(1)
        large_gaps = gaps > 0.01  # Gaps mayores al 1%
        
        if large_gaps.sum() > len(data) * 0.05:  # M√°s del 5% con gaps grandes
            issues.append(f"Demasiados gaps grandes: {large_gaps.sum()}")
        
        # 6. Verificar que no haya precios negativos o cero
        if (data[['Open', 'High', 'Low', 'Close']] <= 0).any().any():
            issues.append("Precios negativos o cero detectados")
        
        # 7. Verificar consistencia OHLC
        invalid_ohlc = (
            (data['High'] < data['Low']) |
            (data['Open'] > data['High']) |
            (data['Close'] > data['High']) |
            (data['Open'] < data['Low']) |
            (data['Close'] < data['Low'])
        )
        
        if invalid_ohlc.sum() > 0:
            issues.append(f"Datos OHLC inconsistentes: {invalid_ohlc.sum()}")
        
        # 8. Verificar que los datos no sean demasiado suaves (indicar√≠a datos sint√©ticos)
        price_smoothness = data['Close'].diff().abs().rolling(10).std()
        if price_smoothness.mean() < 0.0001:  # Muy suave
            issues.append("Datos demasiado suaves - posiblemente sint√©ticos")
        
        if issues:
            logger.warning(f"‚ö†Ô∏è Problemas de realismo en {symbol}:")
            for issue in issues:
                logger.warning(f"   - {issue}")
            return False, issues
        else:
            logger.info(f"‚úÖ Datos de {symbol} validados como realistas")
            return True, []
            
    except Exception as e:
        logger.error(f"‚ùå Error validando realismo de datos: {e}")
        return False, [f"Error de validaci√≥n: {e}"]

UltraForexAI_V2.validate_market_data_realism = validate_market_data_realism

# ===== DETECCI√ìN DE CONCEPT DRIFT =====
def detect_concept_drift(self, model, recent_data, historical_data=None):
    """Detectar concept drift en tiempo real"""
    
    try:
        logger.info("üîç Detectando concept drift...")
        
        # Si no hay datos hist√≥ricos, usar datos recientes divididos
        if historical_data is None:
            split_point = int(0.7 * len(recent_data))
            historical_data = recent_data.iloc[:split_point]
            recent_data = recent_data.iloc[split_point:]
        
        # Evaluar rendimiento hist√≥rico
        historical_perf = self.evaluate_model_performance(model, historical_data)
        
        # Evaluar rendimiento reciente
        recent_perf = self.evaluate_model_performance(model, recent_data)
        
        # Calcular drift score
        drift_score = abs(historical_perf - recent_perf)
        drift_threshold = 0.1  # 10% de diferencia
        
        drift_detected = drift_score > drift_threshold
        
        logger.info(f"üìä Concept Drift Analysis:")
        logger.info(f"   üìà Rendimiento hist√≥rico: {historical_perf:.3f}")
        logger.info(f"   üìâ Rendimiento reciente: {recent_perf:.3f}")
        logger.info(f"   üîç Drift score: {drift_score:.3f}")
        logger.info(f"   üö® Drift detectado: {'S√ç' if drift_detected else 'NO'}")
        
        return {
            'drift_detected': drift_detected,
            'drift_score': drift_score,
            'historical_performance': historical_perf,
            'recent_performance': recent_perf,
            'threshold': drift_threshold
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error detectando concept drift: {e}")
        return {
            'drift_detected': False,
            'drift_score': 0.0,
            'error': str(e)
        }

def evaluate_model_performance(self, model, data):
    """Evaluar rendimiento del modelo en datos espec√≠ficos"""
    
    try:
        # Preparar features
        style_features = self.get_style_features('day_trading')  # Default
        available_features = [f for f in style_features if f in data.columns]
        
        if len(available_features) < 5:
            logger.warning("‚ö†Ô∏è Features insuficientes para evaluaci√≥n")
            return 0.5
        
        X = data[available_features].fillna(0).values
        y = data['Direction'].fillna(1).values
        
        # Verificar que tenemos datos suficientes
        if len(X) < 10:
            return 0.5
        
        # Predecir
        if hasattr(model, 'predict'):
            predictions = model.predict(X)
            accuracy = accuracy_score(y, predictions)
            return accuracy
        else:
            logger.warning("‚ö†Ô∏è Modelo no tiene m√©todo predict")
            return 0.5
            
    except Exception as e:
        logger.error(f"‚ùå Error evaluando modelo: {e}")
        return 0.5

def adaptive_model_retraining(self, symbol, trading_style, drift_info):
    """Re-entrenar modelo adaptativamente basado en concept drift"""
    
    try:
        logger.info(f"üîÑ Re-entrenamiento adaptativo para {symbol} {trading_style}")
        
        # Obtener datos m√°s recientes
        recent_data = self.get_enhanced_data_multi(symbol, trading_style)
        
        if recent_data is None or len(recent_data) < 20:
            logger.warning("‚ö†Ô∏è Datos insuficientes para re-entrenamiento")
            return None
        
        # Ajustar hiperpar√°metros basado en drift
        drift_score = drift_info.get('drift_score', 0)
        
        if drift_score > 0.2:  # Drift severo
            logger.info("üö® Drift severo detectado - Re-entrenamiento completo")
            # Usar par√°metros m√°s conservadores
            style_config = self.get_style_config(trading_style)
            style_config['target_precision'] *= 0.9  # Reducir target
            style_config['max_signals_per_hour'] = max(5, style_config['max_signals_per_hour'] // 2)
        
        elif drift_score > 0.1:  # Drift moderado
            logger.info("‚ö†Ô∏è Drift moderado detectado - Ajuste de par√°metros")
            # Ajustes menores
            style_config = self.get_style_config(trading_style)
            style_config['target_precision'] *= 0.95
        
        # Re-entrenar ensemble
        style_features = self.get_style_features(trading_style)
        available_features = [f for f in style_features if f in recent_data.columns]
        
        X = recent_data[available_features].fillna(0).values
        y = recent_data['Direction'].fillna(1).values
        
        # Crear nuevo ensemble
        new_ensemble = self.create_ensemble_model_for_style(X, y, trading_style)
        
        if new_ensemble:
            logger.info(f"‚úÖ Re-entrenamiento completado: {new_ensemble['accuracy']:.3f}")
            
            # Guardar modelo actualizado
            self.save_ensemble_models(symbol, {trading_style: new_ensemble})
            
            return new_ensemble
        else:
            logger.error("‚ùå Error en re-entrenamiento adaptativo")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Error en re-entrenamiento adaptativo: {e}")
        return None

def continuous_drift_monitoring(self, symbol, trading_style, check_interval_hours=6):
    """Monitoreo continuo de concept drift"""
    
    logger.info(f"üîç Iniciando monitoreo continuo para {symbol} {trading_style}")
    
    def monitor_loop():
        while True:
            try:
                # Cargar modelo actual
                model_file = self.models_dir / symbol / f"{trading_style}_ensemble.pkl"
                
                if not model_file.exists():
                    logger.warning(f"‚ö†Ô∏è Modelo {trading_style} no encontrado para {symbol}")
                    time.sleep(check_interval_hours * 3600)
                    continue
                
                # Cargar ensemble
                with open(model_file, 'rb') as f:
                    ensemble_data = pickle.load(f)
                
                ensemble = ensemble_data['ensemble']
                
                # Obtener datos recientes
                recent_data = self.get_enhanced_data_multi(symbol, trading_style)
                
                if recent_data is None or len(recent_data) < 20:
                    logger.warning("‚ö†Ô∏è Datos insuficientes para monitoreo")
                    time.sleep(check_interval_hours * 3600)
                    continue
                
                # Detectar drift
                drift_info = self.detect_concept_drift(ensemble, recent_data)
                
                if drift_info['drift_detected']:
                    logger.warning(f"üö® Concept drift detectado en {symbol} {trading_style}")
                    
                    # Re-entrenar adaptativamente
                    new_model = self.adaptive_model_retraining(symbol, trading_style, drift_info)
                    
                    if new_model:
                        logger.info(f"‚úÖ Modelo actualizado para {symbol} {trading_style}")
                    else:
                        logger.error(f"‚ùå Error actualizando modelo para {symbol} {trading_style}")
                
                # Esperar hasta pr√≥ximo check
                logger.info(f"üí§ Monitoreo dormir√° {check_interval_hours} horas...")
                time.sleep(check_interval_hours * 3600)
                
            except Exception as e:
                logger.error(f"‚ùå Error en monitoreo continuo: {e}")
                time.sleep(3600)  # Esperar 1 hora antes de reintentar
    
    # Iniciar thread de monitoreo
    monitor_thread = threading.Thread(
        target=monitor_loop,
        daemon=True,
        name=f"DriftMonitor_{symbol}_{trading_style}"
    )
    monitor_thread.start()
    
    logger.info(f"‚úÖ Monitoreo continuo iniciado para {symbol} {trading_style}")

# AGREGAR M√âTODOS DE CONCEPT DRIFT A LA CLASE
UltraForexAI_V2.detect_concept_drift = detect_concept_drift
UltraForexAI_V2.evaluate_model_performance = evaluate_model_performance
UltraForexAI_V2.adaptive_model_retraining = adaptive_model_retraining
UltraForexAI_V2.continuous_drift_monitoring = continuous_drift_monitoring

print("‚úÖ Parte 2 completada - Sistema de obtenci√≥n de datos")
print("üìù M√©todos agregados:")
print("   - get_enhanced_data_multi()")
print("   - resample_to_4h()")
print("   - validate_data_quality()")

# ===== ULTRAFOREXAI V2 - PARTE 3: FEATURES ESPEC√çFICOS =====
print("üîß PARTE 3: Features espec√≠ficos por estilo de trading")

# FEATURES PARA SCALPING (1-5 MINUTOS)
def add_scalping_features(self, data):
    """Features para scalping de alta precisi√≥n"""
    import ta
    
    logger.info("üî• Agregando features para SCALPING...")
    
    try:
        # === ORDER FLOW ANALYSIS ===
        # Bid-Ask spread proxy
        hl_range = data['High'] - data['Low']
        data['bid_ask_spread'] = hl_range / np.maximum(data['Close'], 0.0001)
        data['spread_percentile'] = data['bid_ask_spread'].rolling(100).rank(pct=True)
        
        # Price impact
        oc_change = np.abs(data['Close'] - data['Open'])
        data['price_impact'] = oc_change / np.maximum(data['Volume'], 1)
        data['impact_anomaly'] = (data['price_impact'] > data['price_impact'].rolling(50).quantile(0.8)).astype(int)
        
        # === MICRO MOMENTUM ===
        data['momentum_1'] = data['Close'].pct_change(1)
        data['momentum_3'] = data['Close'].pct_change(3)
        data['momentum_5'] = data['Close'].pct_change(5)
        data['momentum_acceleration'] = data['momentum_1'].diff(1)
        
        # === VOLUME PROFILE ===
        if 'Volume' in data.columns and data['Volume'].notna().sum() > 0:
            volume_ma = data['Volume'].rolling(20).mean()
            data['volume_spike'] = (data['Volume'] > volume_ma * 2).astype(int)
            data['volume_dry'] = (data['Volume'] < volume_ma * 0.5).astype(int)
            data['volume_price_trend'] = data['Volume'] * data['Close'].pct_change()
        else:
            # Fallback para datos sin volumen
            data['volume_spike'] = 0
            data['volume_dry'] = 0
            data['volume_price_trend'] = 0
        
        # === MICROSTRUCTURE PATTERNS ===
        body = np.abs(data['Close'] - data['Open'])
        total_range = np.maximum(hl_range, 0.0001)
        data['doji'] = (body / total_range < 0.1).astype(int)
        data['hammer'] = ((data['Close'] - data['Low']) > 2 * body).astype(int)
        data['shooting_star'] = ((data['High'] - data['Close']) > 2 * body).astype(int)
        
        # === SESSION TIMING ===
        if 'Hour' in data.columns:
            data['london_open'] = (data['Hour'] == 8).astype(int)
            data['ny_open'] = (data['Hour'] == 13).astype(int)
            data['overlap_peak'] = ((data['Hour'] >= 14) & (data['Hour'] <= 15)).astype(int)
        else:
            # Fallback si no hay columna Hour
            data['london_open'] = 0
            data['ny_open'] = 0
            data['overlap_peak'] = 0
        
        # RSI r√°pido para scalping
        data['RSI_fast'] = ta.momentum.RSIIndicator(data['Close'], window=6).rsi()
        
        logger.info("‚úÖ Features scalping agregados correctamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error agregando features scalping: {e}")
        # Agregar features b√°sicos como fallback
        data['momentum_1'] = data['Close'].pct_change(1)
        data['bid_ask_spread'] = (data['High'] - data['Low']) / data['Close']
        data['volume_spike'] = 0
    
    return data

# FEATURES PARA DAY TRADING (15-30 MINUTOS)
def add_day_trading_features(self, data):
    """Features para day trading equilibrado"""
    import ta
    
    logger.info("üìà Agregando features para DAY TRADING...")
    
    try:
        # === MOMENTUM ANALYSIS ===
        data['momentum_30min'] = data['Close'].pct_change(2)  # 2 per√≠odos de 15min
        data['momentum_1h'] = data['Close'].pct_change(4)     # 4 per√≠odos de 15min
        data['momentum_2h'] = data['Close'].pct_change(8)     # 8 per√≠odos de 15min
        
        # RSI multiple timeframes
        data['RSI_14'] = ta.momentum.RSIIndicator(data['Close'], window=14).rsi()
        data['RSI_7'] = ta.momentum.RSIIndicator(data['Close'], window=7).rsi()
        data['RSI_divergence'] = data['RSI_7'] - data['RSI_14']
        
        # === VOLATILITY REGIMES ===
        data['ATR'] = ta.volatility.AverageTrueRange(data['High'], data['Low'], data['Close']).average_true_range()
        data['ATR_percentile'] = data['ATR'].rolling(100).rank(pct=True)
        data['high_vol_regime'] = (data['ATR_percentile'] > 0.8).astype(int)
        data['low_vol_regime'] = (data['ATR_percentile'] < 0.2).astype(int)
        
        # === SUPPORT/RESISTANCE ===
        data['resistance_2h'] = data['High'].rolling(8).max()
        data['support_2h'] = data['Low'].rolling(8).min()
        
        resistance_diff = data['resistance_2h'] - data['Close']
        support_diff = data['Close'] - data['support_2h']
        
        data['distance_to_resistance'] = resistance_diff / np.maximum(data['Close'], 0.0001)
        data['distance_to_support'] = support_diff / np.maximum(data['Close'], 0.0001)
        
        data['resistance_break'] = (data['Close'] > data['resistance_2h'].shift(1)).astype(int)
        data['support_break'] = (data['Close'] < data['support_2h'].shift(1)).astype(int)
        
        # === SESSION PATTERNS ===
        if 'Hour' in data.columns:
            data['london_session'] = ((data['Hour'] >= 8) & (data['Hour'] <= 16)).astype(int)
            data['ny_session'] = ((data['Hour'] >= 13) & (data['Hour'] <= 21)).astype(int)
            data['overlap_session'] = (data['london_session'] & data['ny_session']).astype(int)
        else:
            # Fallback si no hay columna Hour
            data['london_session'] = 0
            data['ny_session'] = 0
            data['overlap_session'] = 0
        
        # === VOLUME ANALYSIS ===
        if 'Volume' in data.columns and data['Volume'].notna().sum() > 0:
            data['volume_ma'] = data['Volume'].rolling(20).mean()
            data['volume_ratio'] = data['Volume'] / np.maximum(data['volume_ma'], 1)
        else:
            # Fallback para datos sin volumen
            data['volume_ma'] = 1
            data['volume_ratio'] = 1
        
        logger.info("‚úÖ Features day trading agregados correctamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error agregando features day trading: {e}")
        # Fallback b√°sico
        data['momentum_1h'] = data['Close'].pct_change(4)
        data['RSI_14'] = ta.momentum.RSIIndicator(data['Close'], window=14).rsi()
        data['ATR'] = ta.volatility.AverageTrueRange(data['High'], data['Low'], data['Close']).average_true_range()
    
    return data

# FEATURES PARA SWING TRADING (1-4 HORAS)
def add_swing_trading_features(self, data):
    """Features para swing trading de tendencias"""
    import ta
    
    logger.info("üìä Agregando features para SWING TRADING...")
    
    try:
        # === TREND ANALYSIS ===
        data['EMA_20'] = ta.trend.EMAIndicator(data['Close'], window=20).ema_indicator()
        data['EMA_50'] = ta.trend.EMAIndicator(data['Close'], window=50).ema_indicator()
        data['EMA_100'] = ta.trend.EMAIndicator(data['Close'], window=100).ema_indicator()
        
        data['trend_strength'] = (data['Close'] - data['EMA_100']) / np.maximum(data['EMA_100'], 0.0001)
        data['trend_acceleration'] = data['EMA_20'] - data['EMA_50']
        
        # === BREAKOUT PATTERNS ===
        bb = ta.volatility.BollingerBands(data['Close'])
        data['BB_upper'] = bb.bollinger_hband()
        data['BB_lower'] = bb.bollinger_lband()
        
        bb_range = data['BB_upper'] - data['BB_lower']
        data['BB_position'] = (data['Close'] - data['BB_lower']) / np.maximum(bb_range, 0.0001)
        
        data['BB_squeeze'] = (bb_range / np.maximum(data['Close'], 0.0001) < 0.02).astype(int)
        data['BB_breakout_up'] = (data['Close'] > data['BB_upper'].shift(1)).astype(int)
        data['BB_breakout_down'] = (data['Close'] < data['BB_lower'].shift(1)).astype(int)
        
        # === MOMENTUM OSCILLATORS ===
        macd = ta.trend.MACD(data['Close'])
        data['MACD'] = macd.macd()
        data['MACD_signal'] = macd.macd_signal()
        data['MACD_histogram'] = macd.macd_diff()
        
        # MACD signals
        macd_cross_up = (data['MACD'] > data['MACD_signal']) & (data['MACD'].shift(1) <= data['MACD_signal'].shift(1))
        macd_cross_down = (data['MACD'] < data['MACD_signal']) & (data['MACD'].shift(1) >= data['MACD_signal'].shift(1))
        
        data['MACD_bullish'] = macd_cross_up.astype(int)
        data['MACD_bearish'] = macd_cross_down.astype(int)
        
        # === WEEKLY PATTERNS ===
        data['monday_effect'] = (data['DayOfWeek'] == 0).astype(int)
        data['friday_effect'] = (data['DayOfWeek'] == 4).astype(int)
        data['mid_week'] = ((data['DayOfWeek'] >= 1) & (data['DayOfWeek'] <= 3)).astype(int)
        
        logger.info("‚úÖ Features swing trading agregados correctamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error agregando features swing trading: {e}")
        # Fallback b√°sico
        data['EMA_20'] = ta.trend.EMAIndicator(data['Close'], window=20).ema_indicator()
        data['trend_strength'] = (data['Close'] - data['EMA_20']) / data['EMA_20']
        data['MACD'] = ta.trend.MACD(data['Close']).macd()
    
    return data

# FEATURES PARA POSITION TRADING (1 D√çA)
def add_position_trading_features(self, data):
    """Features para position trading de largo plazo"""
    import ta
    
    logger.info("üìâ Agregando features para POSITION TRADING...")
    
    try:
        # === MACRO TRENDS ===
        data['SMA_50'] = ta.trend.SMAIndicator(data['Close'], window=50).sma_indicator()
        data['SMA_100'] = ta.trend.SMAIndicator(data['Close'], window=100).sma_indicator()
        data['SMA_200'] = ta.trend.SMAIndicator(data['Close'], window=200).sma_indicator()
        
        data['long_trend'] = np.where(data['Close'] > data['SMA_200'], 1,
                                     np.where(data['Close'] < data['SMA_200'], -1, 0))
        
        # Golden cross / Death cross
        sma50_above_200 = data['SMA_50'] > data['SMA_200']
        sma50_above_200_prev = data['SMA_50'].shift(1) > data['SMA_200'].shift(1)
        
        data['golden_cross'] = (sma50_above_200 & ~sma50_above_200_prev).astype(int)
        data['death_cross'] = (~sma50_above_200 & sma50_above_200_prev).astype(int)
        
        # === MOMENTUM LONG TERM ===
        data['ROC_20'] = ((data['Close'] - data['Close'].shift(20)) / np.maximum(data['Close'].shift(20), 0.0001)) * 100
        data['ROC_50'] = ((data['Close'] - data['Close'].shift(50)) / np.maximum(data['Close'].shift(50), 0.0001)) * 100
        
        # === VOLATILITY LONG TERM ===
        data['volatility_20'] = data['Close'].pct_change().rolling(20).std()
        data['volatility_regime'] = data['volatility_20'].rolling(252).rank(pct=True)
        
        # === SEASONAL PATTERNS ===
        data['january_effect'] = (data['Month'] == 1).astype(int)
        data['december_effect'] = (data['Month'] == 12).astype(int)
        data['quarter_end'] = ((data['Month'] % 3) == 0).astype(int)
        
        # === FUNDAMENTAL PROXIES ===
        data['price_momentum_long'] = data['Close'].pct_change(21)  # 21 d√≠as = 1 mes
        data['momentum_percentile'] = data['price_momentum_long'].rolling(252).rank(pct=True)
        
        logger.info("‚úÖ Features position trading agregados correctamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error agregando features position trading: {e}")
        # Fallback b√°sico
        data['SMA_50'] = ta.trend.SMAIndicator(data['Close'], window=50).sma_indicator()
        data['long_trend'] = np.where(data['Close'] > data['SMA_50'], 1, -1)
        data['ROC_20'] = data['Close'].pct_change(20) * 100
    
    return data

def get_style_features(self, trading_style):
    """Obtener features espec√≠ficos para cada estilo"""
    
    feature_sets = {
        'scalping': [
            # Microestructura
            'bid_ask_spread', 'spread_percentile', 'price_impact', 'impact_anomaly',
            # Momentum micro
            'momentum_1', 'momentum_3', 'momentum_5', 'momentum_acceleration',
            # Volume profile
            'volume_spike', 'volume_dry', 'volume_price_trend',
            # Patterns
            'doji', 'hammer', 'shooting_star',
            # Timing
            'london_open', 'ny_open', 'overlap_peak', 
            # Technical
            'RSI_fast', 'Hour'
        ],
        
        'day_trading': [
            # Momentum
            'momentum_30min', 'momentum_1h', 'momentum_2h',
            'RSI_14', 'RSI_7', 'RSI_divergence',
            # Volatilidad
            'ATR', 'ATR_percentile', 'high_vol_regime', 'low_vol_regime',
            # Support/Resistance
            'distance_to_resistance', 'distance_to_support',
            'resistance_break', 'support_break',
            # Sessions
            'london_session', 'ny_session', 'overlap_session',
            # Volume
            'volume_ratio',
            # Time
            'Hour', 'DayOfWeek'
        ],
        
        'swing_trading': [
            # Trend
            'EMA_20', 'EMA_50', 'EMA_100', 'trend_strength', 'trend_acceleration',
            # Bollinger Bands
            'BB_position', 'BB_squeeze', 'BB_breakout_up', 'BB_breakout_down',
            # MACD
            'MACD', 'MACD_signal', 'MACD_histogram', 'MACD_bullish', 'MACD_bearish',
            # Patterns
            'monday_effect', 'friday_effect', 'mid_week',
            'DayOfWeek', 'Hour'
        ],
        
        'position_trading': [
            # Long-term trend
            'SMA_50', 'SMA_100', 'SMA_200', 'long_trend',
            'golden_cross', 'death_cross',
            # Momentum
            'ROC_20', 'ROC_50', 'momentum_percentile',
            # Volatility
            'volatility_20', 'volatility_regime',
            # Seasonal
            'january_effect', 'december_effect', 'quarter_end',
            'Month', 'DayOfWeek',
            # Fundamental proxies
            'price_momentum_long'
        ]
    }
    
    features = feature_sets.get(trading_style, feature_sets['day_trading'])
    logger.info(f"üìã Features para {trading_style}: {len(features)} seleccionados")
    
    return features

# AGREGAR ESTOS M√âTODOS A LA CLASE
UltraForexAI_V2.add_scalping_features = add_scalping_features
UltraForexAI_V2.add_day_trading_features = add_day_trading_features
UltraForexAI_V2.add_swing_trading_features = add_swing_trading_features
UltraForexAI_V2.add_position_trading_features = add_position_trading_features
UltraForexAI_V2.get_style_features = get_style_features

print("‚úÖ Parte 3 completada - Features espec√≠ficos por estilo")
print("üìù M√©todos agregados:")
print("   - add_scalping_features() - 16 features √∫nicos")
print("   - add_day_trading_features() - 17 features balanceados") 
print("   - add_swing_trading_features() - 15 features de tendencia")
print("   - add_position_trading_features() - 15 features macro")
print("   - get_style_features() - Selector autom√°tico")

# ===== ULTRAFOREXAI V2 - PARTE 4: TARGETS ADAPTATIVOS MEJORADOS =====
print("üéØ PARTE 4: Sistema de targets adaptativos mejorado")

def create_adaptive_target(self, data, config):
    """Target adaptativo basado en volatilidad y percentiles din√°micos"""
    
    try:
        logger.info("üéØ Creando target adaptativo...")
        
        # Calcular volatilidad din√°mica
        volatility = data['Close'].pct_change().rolling(20).std()
        avg_volatility = volatility.rolling(50).mean()
        
        # Umbrales adaptativos basados en volatilidad
        volatility_percentile = volatility.rolling(100).rank(pct=True)
        
        # Usar percentiles din√°micos para crear umbrales
        pct_25 = data['Close'].pct_change().rolling(50).quantile(0.25)
        pct_75 = data['Close'].pct_change().rolling(50).quantile(0.75)
        
        # Umbrales adaptativos
        buy_threshold = np.maximum(pct_75 * 0.8, avg_volatility * 1.5)
        sell_threshold = np.minimum(pct_25 * 0.8, -avg_volatility * 1.5)
        
        # Crear target balanceado
        target = self.create_balanced_target(data, buy_threshold, sell_threshold)
        
        return target
        
    except Exception as e:
        logger.error(f"‚ùå Error en target adaptativo: {e}")
        return self.create_fallback_target(data)

def create_balanced_target(self, data, buy_threshold, sell_threshold):
    """Crear target balanceado con distribuci√≥n equilibrada - VERSI√ìN MEJORADA"""
    
    try:
        # Calcular returns futuros seg√∫n el estilo
        horizon_mapping = {
            'scalping': 5,      # 5 per√≠odos hacia adelante
            'day_trading': 10,  # 10 per√≠odos hacia adelante
            'swing_trading': 20, # 20 per√≠odos hacia adelante
            'position_trading': 50 # 50 per√≠odos hacia adelante
        }
        
        # Determinar horizonte basado en el contexto
        horizon = 10  # Default
        if hasattr(self, 'current_trading_style'):
            horizon = horizon_mapping.get(self.current_trading_style, 10)
        
        future_returns = data['Close'].shift(-horizon) / data['Close'] - 1
        
        # Distribuciones objetivo m√°s equilibradas (como en el archivo exitoso)
        target_distributions = {
            'scalping': {'BUY': 0.25, 'SELL': 0.25, 'HOLD': 0.50},
            'day_trading': {'BUY': 0.25, 'SELL': 0.25, 'HOLD': 0.50},
            'swing_trading': {'BUY': 0.20, 'SELL': 0.20, 'HOLD': 0.60},
            'position_trading': {'BUY': 0.15, 'SELL': 0.15, 'HOLD': 0.70}
        }
        
        # Usar distribuci√≥n objetivo del estilo actual
        current_style = getattr(self, 'current_trading_style', 'day_trading')
        target_dist = target_distributions.get(current_style, {'BUY': 0.25, 'SELL': 0.25, 'HOLD': 0.50})
        
        # Calcular percentiles para distribuci√≥n objetivo
        buy_pct = 100 * (1 - target_dist['BUY'])
        sell_pct = 100 * target_dist['SELL']
        
        # Usar percentiles en lugar de umbrales fijos
        buy_threshold = np.percentile(future_returns.dropna(), buy_pct)
        sell_threshold = np.percentile(future_returns.dropna(), sell_pct)
        
        # Crear target con percentiles
        target = np.where(future_returns > buy_threshold, 2,  # BUY
                         np.where(future_returns < sell_threshold, 0,  # SELL
                                 1))  # HOLD
        
        # Verificar distribuci√≥n
        unique_classes = np.unique(target)
        class_counts = np.bincount(target)
        
        logger.info(f"üìä Distribuci√≥n inicial: {dict(zip(unique_classes, class_counts))}")
        
        # Balancear distribuci√≥n si es necesario
        if len(class_counts) >= 3:
            current_dist = {
                'BUY': class_counts[2] / len(target),
                'SELL': class_counts[0] / len(target),
                'HOLD': class_counts[1] / len(target)
            }
            
            # Ajustar si hay desbalance extremo
            max_ratio = max(current_dist.values())
            min_ratio = min(current_dist.values())
            
            if max_ratio > 0.6:  # M√°s del 60% en una clase
                logger.info("üîÑ Ajustando distribuci√≥n - desbalance detectado")
                
                # Recalcular con percentiles m√°s equilibrados
                buy_pct = 70  # 30% BUY
                sell_pct = 30  # 30% SELL
                
                buy_threshold = np.percentile(future_returns.dropna(), buy_pct)
                sell_threshold = np.percentile(future_returns.dropna(), sell_pct)
                
                target = np.where(future_returns > buy_threshold, 2,
                                np.where(future_returns < sell_threshold, 0, 1))
        
        # Verificar resultado final
        final_classes = np.unique(target)
        final_counts = np.bincount(target)
        
        logger.info(f"‚úÖ Target balanceado creado: {len(final_classes)} clases")
        logger.info(f"üìä Distribuci√≥n final: {dict(zip(final_classes, final_counts))}")
        
        return target
        
    except Exception as e:
        logger.error(f"‚ùå Error en target balanceado: {e}")
        return self.create_fallback_target(data)

def create_style_target(self, data, trading_style):
    """Crear target espec√≠fico para cada estilo de trading - VERSI√ìN MEJORADA"""
    
    style_config = self.get_style_config(trading_style)
    
    logger.info(f"üéØ Creando target mejorado para {trading_style}...")
    
    # Guardar el estilo actual para usar en create_balanced_target
    self.current_trading_style = trading_style
    
    # Usar target adaptativo como base
    adaptive_target = self.create_adaptive_target(data, style_config)
    
    if adaptive_target is not None:
        # Aplicar ajustes espec√≠ficos por estilo
        if trading_style == 'scalping':
            return self.adjust_target_for_scalping(adaptive_target, data, style_config)
        elif trading_style == 'day_trading':
            return self.adjust_target_for_day_trading(adaptive_target, data, style_config)
        elif trading_style == 'swing_trading':
            return self.adjust_target_for_swing_trading(adaptive_target, data, style_config)
        elif trading_style == 'position_trading':
            return self.adjust_target_for_position_trading(adaptive_target, data, style_config)
        else:
            return adaptive_target
    
    return self.create_fallback_target(data)

def adjust_target_for_scalping(self, base_target, data, config):
    """Ajustar target para scalping - m√°s se√±ales, menor precisi√≥n"""
    
    try:
        # Para scalping, queremos m√°s se√±ales
        volatility = data['Close'].pct_change().rolling(10).std()
        
        # Crear se√±ales adicionales basadas en momentum
        momentum_5 = data['Close'].pct_change(5)
        momentum_10 = data['Close'].pct_change(10)
        
        # Se√±ales de momentum
        momentum_buy = (momentum_5 > volatility * 2) & (momentum_10 > 0)
        momentum_sell = (momentum_5 < -volatility * 2) & (momentum_10 < 0)
        
        # Combinar con target base
        adjusted_target = np.where(momentum_buy, 2,
                                 np.where(momentum_sell, 0, base_target))
        
        # Verificar distribuci√≥n
        unique_classes = np.unique(adjusted_target)
        logger.info(f"‚úÖ Scalping target ajustado: {len(unique_classes)} clases")
        
        return adjusted_target
        
    except Exception as e:
        logger.error(f"‚ùå Error ajustando target scalping: {e}")
        return base_target

def adjust_target_for_day_trading(self, base_target, data, config):
    """Ajustar target para day trading - balance entre se√±ales y precisi√≥n"""
    
    try:
        # Para day trading, usar m√∫ltiples timeframes
        returns_1h = data['Close'].pct_change(4)  # 4 per√≠odos de 15min
        returns_2h = data['Close'].pct_change(8)
        returns_4h = data['Close'].pct_change(16)
        
        # Consenso de m√∫ltiples timeframes
        consensus_buy = (returns_1h > 0) & (returns_2h > 0) & (returns_4h > 0)
        consensus_sell = (returns_1h < 0) & (returns_2h < 0) & (returns_4h < 0)
        
        # Combinar con target base
        adjusted_target = np.where(consensus_buy, 2,
                                 np.where(consensus_sell, 0, base_target))
        
        unique_classes = np.unique(adjusted_target)
        logger.info(f"‚úÖ Day trading target ajustado: {len(unique_classes)} clases")
        
        return adjusted_target
        
    except Exception as e:
        logger.error(f"‚ùå Error ajustando target day trading: {e}")
        return base_target

def adjust_target_for_swing_trading(self, base_target, data, config):
    """Ajustar target para swing trading - enfocado en tendencias"""
    
    try:
        # Para swing trading, usar tendencias m√°s largas
        sma_20 = data['Close'].rolling(20).mean()
        sma_50 = data['Close'].rolling(50).mean()
        
        # Se√±ales de tendencia
        uptrend = (data['Close'] > sma_20) & (sma_20 > sma_50)
        downtrend = (data['Close'] < sma_20) & (sma_20 < sma_50)
        
        # Combinar con target base
        adjusted_target = np.where(uptrend, 2,
                                 np.where(downtrend, 0, base_target))
        
        unique_classes = np.unique(adjusted_target)
        logger.info(f"‚úÖ Swing trading target ajustado: {len(unique_classes)} clases")
        
        return adjusted_target
        
    except Exception as e:
        logger.error(f"‚ùå Error ajustando target swing trading: {e}")
        return base_target

def adjust_target_for_position_trading(self, base_target, data, config):
    """Ajustar target para position trading - cambios de tendencia mayor"""
    
    try:
        # Para position trading, usar indicadores de largo plazo
        sma_50 = data['Close'].rolling(50).mean()
        sma_200 = data['Close'].rolling(200).mean()
        
        # Cambios de tendencia
        golden_cross = (sma_50 > sma_200) & (sma_50.shift(1) <= sma_200.shift(1))
        death_cross = (sma_50 < sma_200) & (sma_50.shift(1) >= sma_200.shift(1))
        
        # Combinar con target base
        adjusted_target = np.where(golden_cross, 2,
                                 np.where(death_cross, 0, base_target))
        
        unique_classes = np.unique(adjusted_target)
        logger.info(f"‚úÖ Position trading target ajustado: {len(unique_classes)} clases")
        
        return adjusted_target
        
    except Exception as e:
        logger.error(f"‚ùå Error ajustando target position trading: {e}")
        return base_target

# REEMPLAZAR M√âTODOS ANTIGUOS CON VERSI√ìN MEJORADA
def create_scalping_target(self, data, config):
    """Target para scalping - VERSI√ìN MEJORADA"""
    return self.create_adaptive_target(data, config)

def create_day_trading_target(self, data, config):
    """Target para day trading - VERSI√ìN MEJORADA"""
    return self.create_adaptive_target(data, config)

def create_swing_trading_target(self, data, config):
    """Target para swing trading - VERSI√ìN MEJORADA"""
    return self.create_adaptive_target(data, config)

def create_position_trading_target(self, data, config):
    """Target para position trading - VERSI√ìN MEJORADA"""
    return self.create_adaptive_target(data, config)

def create_adaptive_quality_filter(self, data):
    """Crear filtro de calidad adaptativo - VERSI√ìN MEJORADA"""
    
    try:
        # Filtros m√°s permisivos para evitar targets con una sola clase
        filters = []
        
        # 1. Precio v√°lido (muy permisivo)
        if 'Close' in data.columns:
            valid_price = (data['Close'] > 0) & (data['Close'].notna())
            filters.append(valid_price)
        
        # 2. Volatilidad m√≠nima (muy permisivo)
        if 'High' in data.columns and 'Low' in data.columns:
            price_range = (data['High'] - data['Low']) / data['Close']
            min_volatility = price_range > 0.00001  # Extremadamente permisivo
            filters.append(min_volatility)
        
        # 3. Momentum b√°sico (muy permisivo)
        if 'Close' in data.columns:
            momentum = data['Close'].pct_change().abs()
            min_momentum = momentum > momentum.rolling(50).quantile(0.01)  # Solo 1% m√°s bajo
            filters.append(min_momentum)
        
        # Combinar filtros
        if filters:
            quality_filter = pd.concat(filters, axis=1).all(axis=1)
        else:
            quality_filter = pd.Series([True] * len(data), index=data.index)
        
        # Asegurar que al menos 80% de los datos pasen el filtro
        filter_ratio = quality_filter.mean()
        if filter_ratio < 0.8:
            logger.warning(f"‚ö†Ô∏è Filtro muy restrictivo: {filter_ratio:.1%}, relajando...")
            # Relajar filtro significativamente
            quality_filter = pd.Series([True] * len(data), index=data.index)
        
        return quality_filter
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error en filtro de calidad: {e}")
        return pd.Series([True] * len(data), index=data.index)

def create_fallback_target(self, data):
    """Target de fallback mejorado - VERSI√ìN MEJORADA"""
    
    try:
        logger.info("üîÑ Creando target de fallback mejorado...")
        
        # Target b√°sico basado en retornos futuros
        future_return = data['Close'].shift(-5) / data['Close'] - 1
        
        # Umbrales muy permisivos para asegurar variabilidad
        volatility = data['Close'].pct_change().rolling(20).std()
        threshold = volatility.rolling(50).mean() * 0.5  # Reducido de 2 a 0.5
        
        # Target con umbrales muy permisivos
        target = np.where(future_return > threshold, 2,  # BUY
                         np.where(future_return < -threshold, 0,  # SELL
                                 1))  # HOLD
        
        # Si a√∫n no hay variabilidad, usar percentiles
        unique_classes = np.unique(target)
        if len(unique_classes) < 2:
            logger.warning("‚ö†Ô∏è Target fallback sin variabilidad, usando percentiles")
            
            # Usar percentiles para crear clases
            pct_33 = np.percentile(future_return.dropna(), 33)
            pct_66 = np.percentile(future_return.dropna(), 66)
            
            target = np.where(future_return > pct_66, 2,
                             np.where(future_return < pct_33, 0, 1))
        
        # Si a√∫n no hay variabilidad, usar umbral fijo muy bajo
        unique_classes = np.unique(target)
        if len(unique_classes) < 2:
            logger.warning("‚ö†Ô∏è Target fallback a√∫n sin variabilidad, usando umbral fijo muy bajo")
            fixed_threshold = 0.0001  # Extremadamente bajo
            target = np.where(future_return > fixed_threshold, 2,
                             np.where(future_return < -fixed_threshold, 0, 1))
        
        logger.info(f"‚úÖ Target fallback mejorado creado: {len(np.unique(target))} clases")
        return target
        
    except Exception as e:
        logger.error(f"‚ùå Error en target fallback: {e}")
        # Target m√°s b√°sico posible con variabilidad garantizada
        return np.array([0, 1, 2] * (len(data) // 3 + 1))[:len(data)]

def fix_feature_variance_issues(self, data):
    """Corregir problemas de varianza en features"""
    
    logger.info("üîß Corrigiendo problemas de varianza en features...")
    
    try:
        # Identificar features con varianza cero
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        zero_variance_features = []
        
        for col in numeric_columns:
            if data[col].var() == 0:
                zero_variance_features.append(col)
        
        if zero_variance_features:
            logger.warning(f"‚ö†Ô∏è Features con varianza cero: {zero_variance_features}")
            
            # Remover features problem√°ticas
            data = data.drop(columns=zero_variance_features)
            logger.info(f"‚úÖ Features removidas: {len(zero_variance_features)}")
        
        # Crear features alternativos si es necesario
        if 'Volume' not in data.columns or data['Volume'].var() == 0:
            # Crear proxy de volumen basado en volatilidad
            data['volume_proxy'] = data['Close'].pct_change().abs()
            logger.info("‚úÖ Creado proxy de volumen")
        
        if 'Hour' not in data.columns:
            # Crear features de tiempo b√°sicos
            data['hour'] = pd.to_datetime(data.index).hour
            data['is_weekend'] = pd.to_datetime(data.index).weekday >= 5
            logger.info("‚úÖ Creados features de tiempo b√°sicos")
        
        return data
        
    except Exception as e:
        logger.error(f"‚ùå Error corrigiendo features: {e}")
        return data

# AGREGAR M√âTODOS MEJORADOS A LA CLASE
UltraForexAI_V2.create_style_target = create_style_target  # ‚Üê AGREGADA L√çNEA FALTANTE
UltraForexAI_V2.create_adaptive_target = create_adaptive_target
UltraForexAI_V2.create_balanced_target = create_balanced_target
UltraForexAI_V2.adjust_target_for_scalping = adjust_target_for_scalping
UltraForexAI_V2.adjust_target_for_day_trading = adjust_target_for_day_trading
UltraForexAI_V2.adjust_target_for_swing_trading = adjust_target_for_swing_trading
UltraForexAI_V2.adjust_target_for_position_trading = adjust_target_for_position_trading
UltraForexAI_V2.fix_feature_variance_issues = fix_feature_variance_issues

print("‚úÖ Parte 4 completada - Targets adaptativos mejorados")
print("üìù M√©todos agregados:")
print("   - create_adaptive_target() - Targets basados en volatilidad")
print("   - create_balanced_target() - Distribuci√≥n equilibrada")
print("   - adjust_target_for_*() - Ajustes espec√≠ficos por estilo")
print("   - create_adaptive_quality_filter() - Filtros m√°s permisivos")
print("   - create_fallback_target() - Fallback mejorado")
print("üõ°Ô∏è Caracter√≠sticas mejoradas:")
print("   ‚úÖ Umbrales adaptativos basados en volatilidad")
print("   ‚úÖ Percentiles din√°micos para distribuci√≥n balanceada")
print("   ‚úÖ Filtros m√°s permisivos para evitar una sola clase")
print("   ‚úÖ Ajustes espec√≠ficos por estilo de trading")
print("   ‚úÖ Fallbacks robustos con variabilidad garantizada")

# ===== ULTRAFOREXAI V2 - PARTE 5: OPTIMIZACI√ìN Y ENTRENAMIENTO (RECREADA) =====
print("üîß PARTE 5: Sistema de optimizaci√≥n y entrenamiento de modelos (RECREADA)")

def optimize_hyperparameters_for_style(self, X, y, model_type, trading_style):
    """Optimizaci√≥n de hiperpar√°metros espec√≠fica por estilo"""
    
    try:
        import optuna
        from sklearn.model_selection import cross_val_score, TimeSeriesSplit
        
        style_config = self.get_style_config(trading_style)
        target_precision = style_config['target_precision']
        
        logger.info(f"üîß Optimizando {model_type} para {trading_style} (target: {target_precision:.1%})")
        
        def objective(trial):
            try:
                if model_type == 'RandomForest':
                    from sklearn.ensemble import RandomForestClassifier
                    
                    # Par√°metros espec√≠ficos por estilo
                    if trading_style == 'scalping':
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                            'max_depth': trial.suggest_int('max_depth', 3, 8),
                            'min_samples_split': trial.suggest_int('min_samples_split', 10, 50),
                            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 25),
                            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
                            'random_state': 42,
                            'n_jobs': -1
                        }
                    elif trading_style == 'day_trading':
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 100, 300),
                            'max_depth': trial.suggest_int('max_depth', 5, 12),
                            'min_samples_split': trial.suggest_int('min_samples_split', 5, 30),
                            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 15),
                            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                            'random_state': 42,
                            'n_jobs': -1
                        }
                    elif trading_style == 'swing_trading':
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 200, 500),
                            'max_depth': trial.suggest_int('max_depth', 8, 15),
                            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                            'random_state': 42,
                            'n_jobs': -1
                        }
                    else:  # position_trading
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 300, 600),
                            'max_depth': trial.suggest_int('max_depth', 10, 20),
                            'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),
                            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 8),
                            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                            'random_state': 42,
                            'n_jobs': -1
                        }
                    model = RandomForestClassifier(**params)
                    
                elif model_type == 'XGBoost':
                    import xgboost as xgb
                    
                    if trading_style == 'scalping':
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 50, 150),
                            'max_depth': trial.suggest_int('max_depth', 2, 6),
                            'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),
                            'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                            'random_state': 42,
                            'n_jobs': -1,
                            'eval_metric': 'mlogloss'
                        }
                    else:
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 100, 400),
                            'max_depth': trial.suggest_int('max_depth', 3, 10),
                            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                            'random_state': 42,
                            'n_jobs': -1,
                            'eval_metric': 'mlogloss'
                        }
                    
                    model = xgb.XGBClassifier(**params)
                
                elif model_type == 'LightGBM':
                    import lightgbm as lgb
                    
                    if trading_style == 'scalping':
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 50, 150),
                            'max_depth': trial.suggest_int('max_depth', 2, 6),
                            'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),
                            'num_leaves': trial.suggest_int('num_leaves', 10, 50),
                            'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                            'random_state': 42,
                            'n_jobs': -1,
                            'verbose': -1
                        }
                    else:
                        params = {
                            'n_estimators': trial.suggest_int('n_estimators', 100, 400),
                            'max_depth': trial.suggest_int('max_depth', 3, 10),
                            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                            'num_leaves': trial.suggest_int('num_leaves', 10, 100),
                            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                            'random_state': 42,
                            'n_jobs': -1,
                            'verbose': -1
                        }
                    
                    model = lgb.LGBMClassifier(**params)
                
                # Cross-validation temporal
                if trading_style in ['scalping', 'day_trading']:
                    cv = TimeSeriesSplit(n_splits=3)
                else:
                    cv = TimeSeriesSplit(n_splits=5)
                
                # Evaluar con manejo de errores
                try:
                    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=1)
                    return cv_scores.mean()
                except Exception as cv_error:
                    logger.warning(f"‚ö†Ô∏è CV error en trial: {cv_error}")
                    return 0.5
                
            except Exception as model_error:
                logger.warning(f"‚ö†Ô∏è Model error en trial: {model_error}")
                return 0.5
        
        # Trials espec√≠ficos por estilo como en el script exitoso
        if trading_style == 'scalping':
            n_trials = 25
        else:
            n_trials = min(50, self.system_config['hyperopt_trials'])
        
        # Optimizar con manejo de errores y timeout
        try:
            study = optuna.create_study(direction='maximize')
            study.optimize(objective, n_trials=n_trials, timeout=1800)  # 30 min max
            
            if study.best_trial is None:
                logger.warning(f"‚ö†Ô∏è No se complet√≥ optimizaci√≥n para {model_type}")
                return self.get_default_params(model_type, trading_style)
            
            best_score = study.best_value
            best_params = study.best_params
            
            logger.info(f"‚úÖ {model_type} {trading_style} optimizado: {best_score:.3f}")
            
            return best_params
            
        except Exception as optuna_error:
            logger.error(f"‚ùå Error en optimizaci√≥n {model_type}: {optuna_error}")
            return self.get_default_params(model_type, trading_style)
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico optimizando {model_type} para {trading_style}: {e}")
        return self.get_default_params(model_type, trading_style)

def get_default_params(self, model_type, trading_style):
    """Par√°metros por defecto seguros si falla la optimizaci√≥n"""
    
    logger.info(f"üîß Usando par√°metros por defecto para {model_type} {trading_style}")
    
    default_params = {
        'RandomForest': {
            'scalping': {
                'n_estimators': 100, 
                'max_depth': 6, 
                'min_samples_leaf': 10, 
                'max_features': 'sqrt',
                'random_state': 42, 
                'n_jobs': -1
            },
            'day_trading': {
                'n_estimators': 200, 
                'max_depth': 8, 
                'min_samples_leaf': 5,
                'max_features': 'sqrt',
                'random_state': 42, 
                'n_jobs': -1
            },
            'swing_trading': {
                'n_estimators': 300, 
                'max_depth': 12, 
                'min_samples_leaf': 3,
                'max_features': 'log2',
                'random_state': 42, 
                'n_jobs': -1
            },
            'position_trading': {
                'n_estimators': 400, 
                'max_depth': 15, 
                'min_samples_leaf': 2,
                'max_features': None,
                'random_state': 42, 
                'n_jobs': -1
            }
        },
        'XGBoost': {
            'scalping': {
                'n_estimators': 100, 
                'max_depth': 4, 
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1,
                'eval_metric': 'mlogloss'
            },
            'day_trading': {
                'n_estimators': 200, 
                'max_depth': 6, 
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1,
                'eval_metric': 'mlogloss'
            },
            'swing_trading': {
                'n_estimators': 300, 
                'max_depth': 8, 
                'learning_rate': 0.03,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1,
                'eval_metric': 'mlogloss'
            },
            'position_trading': {
                'n_estimators': 400, 
                'max_depth': 10, 
                'learning_rate': 0.02,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1,
                'eval_metric': 'mlogloss'
            }
        },
        'LightGBM': {
            'scalping': {
                'n_estimators': 100, 
                'max_depth': 4, 
                'learning_rate': 0.1,
                'num_leaves': 31,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1, 
                'verbose': -1
            },
            'day_trading': {
                'n_estimators': 200, 
                'max_depth': 6, 
                'learning_rate': 0.05,
                'num_leaves': 31,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1, 
                'verbose': -1
            },
            'swing_trading': {
                'n_estimators': 300, 
                'max_depth': 8, 
                'learning_rate': 0.03,
                'num_leaves': 31,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1, 
                'verbose': -1
            },
            'position_trading': {
                'n_estimators': 400, 
                'max_depth': 10, 
                'learning_rate': 0.02,
                'num_leaves': 31,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42, 
                'n_jobs': -1, 
                'verbose': -1
            }
        }
    }
    
    return default_params.get(model_type, {}).get(trading_style, {'random_state': 42})

def train_optimized_model(self, X, y, model_type, best_params):
    """Entrenar modelo con par√°metros optimizados y validaci√≥n robusta"""
    
    try:
        from sklearn.metrics import accuracy_score
        
        logger.info(f"üöÄ Entrenando {model_type} con par√°metros optimizados...")
        
        # Verificar formato de datos
        if len(X.shape) == 1:
            X = X.reshape(-1, 1)
        
        # Verificar que tenemos datos suficientes
        if len(X) < 5:  # REDUCIDO para permitir scalping
            logger.warning(f"‚ö†Ô∏è Datos insuficientes para {model_type}: {len(X)} muestras")
            return None
        
        # Split temporal estricto
        split_point = int(0.8 * len(X))
        X_train, X_test = X[:split_point], X[split_point:]
        y_train, y_test = y[:split_point], y[split_point:]
        
        # Verificar splits
        if len(X_train) < 5 or len(X_test) < 1:  # M√ÅS PERMISIVO para scalping
            logger.warning(f"‚ö†Ô∏è Splits insuficientes: train={len(X_train)}, test={len(X_test)}")
            return None
        
        # Verificar distribuci√≥n de clases
        unique_train = np.unique(y_train)
        unique_test = np.unique(y_test)
        
        if len(unique_train) < 2:
            logger.warning(f"‚ö†Ô∏è Solo una clase en training: {unique_train}")
            # Intentar crear m√°s variabilidad en el target
            if len(y_train) > 10:
                # Usar percentiles para crear clases adicionales
                pct_33 = np.percentile(y_train, 33)
                pct_66 = np.percentile(y_train, 66)
                
                # Crear clases basadas en percentiles
                y_train_modified = np.where(y_train > pct_66, 2,
                                          np.where(y_train < pct_33, 0, 1))
                y_test_modified = np.where(y_test > pct_66, 2,
                                         np.where(y_test < pct_33, 0, 1))
                
                unique_train_modified = np.unique(y_train_modified)
                if len(unique_train_modified) >= 2:
                    logger.info(f"üîÑ Target modificado con clases: {unique_train_modified}")
                    y_train = y_train_modified
                    y_test = y_test_modified
                    unique_train = unique_train_modified
                else:
                    logger.error(f"‚ùå No se pudo crear variabilidad en target")
                    return None
            else:
                logger.error(f"‚ùå Datos insuficientes para crear variabilidad")
                return None
        
        # Crear modelo con par√°metros
        try:
            if model_type == 'RandomForest':
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(**best_params)
                
            elif model_type == 'XGBoost':
                import xgboost as xgb
                model = xgb.XGBClassifier(**best_params)
                
            elif model_type == 'LightGBM':
                import lightgbm as lgb
                model = lgb.LGBMClassifier(**best_params)
            
        except Exception as model_error:
            logger.error(f"‚ùå Error creando modelo {model_type}: {model_error}")
            return None
        
        # Entrenar con medici√≥n de tiempo
        start_time = time.time()
        
        try:
            model.fit(X_train, y_train)
            training_time = time.time() - start_time
            
        except Exception as fit_error:
            logger.error(f"‚ùå Error entrenando {model_type}: {fit_error}")
            return None
        
        # Evaluar modelo
        try:
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            # Evaluar solo en se√±ales (no HOLD) si es multiclase
            if len(unique_train) > 2:
                signal_mask = y_test != 1
                if signal_mask.sum() > 0:
                    signal_accuracy = accuracy_score(y_test[signal_mask], y_pred[signal_mask])
                else:
                    signal_accuracy = accuracy
            else:
                signal_accuracy = accuracy
            
            logger.info(f"‚úÖ {model_type} entrenado - Precisi√≥n: {accuracy:.2%} (se√±ales: {signal_accuracy:.2%}, tiempo: {training_time:.1f}s)")
            
            return {
                'model': model,
                'accuracy': accuracy,
                'signal_accuracy': signal_accuracy,
                'training_time': training_time,
                'best_params': best_params,
                'feature_importance': getattr(model, 'feature_importances_', None),
                'train_size': len(X_train),
                'test_size': len(X_test),
                'classes_train': unique_train,
                'classes_test': unique_test
            }
            
        except Exception as eval_error:
            logger.error(f"‚ùå Error evaluando {model_type}: {eval_error}")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico entrenando {model_type}: {e}")
        return None

def train_deep_neural_network_for_style(self, X, y, input_shape, trading_style):
    """Entrenar red neuronal espec√≠fica para el estilo"""
    
    try:
        from sklearn.preprocessing import RobustScaler
        
        logger.info(f"üß† Entrenando red neuronal para {trading_style}...")
        
        # Verificar datos m√≠nimos
        if len(X) < 5:  # REDUCIDO para permitir scalping
            logger.warning(f"‚ö†Ô∏è Datos insuficientes para NN: {len(X)}")
            return None
        
        # Preparar datos
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Split temporal
        split_point = int(0.8 * len(X_scaled))
        X_train, X_test = X_scaled[:split_point], X_scaled[split_point:]
        y_train, y_test = y[:split_point], y[split_point:]
        
        # Verificar clases
        unique_classes = np.unique(y_train)
        n_classes = len(unique_classes)
        
        if n_classes < 2:
            logger.warning(f"‚ö†Ô∏è Clases insuficientes para NN: {unique_classes}")
            return None
        
        logger.info(f"üß† NN para {trading_style}: {len(X_train)} train, {len(X_test)} test, {n_classes} clases")
        
        # Arquitectura espec√≠fica por estilo
        try:
            if trading_style == 'scalping':
                # Modelo simple y r√°pido
                model = tf.keras.Sequential([
                    tf.keras.layers.LayerNormalization(input_shape=(input_shape,)),
                    tf.keras.layers.Dense(64, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.5),
                    tf.keras.layers.Dense(32, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.4),
                    tf.keras.layers.Dense(n_classes, activation='softmax')
                ])
            
            elif trading_style == 'day_trading':
                # Modelo balanceado
                model = tf.keras.Sequential([
                    tf.keras.layers.LayerNormalization(input_shape=(input_shape,)),
                    tf.keras.layers.Dense(128, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.4),
                    tf.keras.layers.Dense(64, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.3),
                    tf.keras.layers.Dense(32, activation='swish'),
                    tf.keras.layers.Dropout(0.2),
                    tf.keras.layers.Dense(n_classes, activation='softmax')
                ])
            
            else:  # swing_trading, position_trading
                # Modelo m√°s complejo
                model = tf.keras.Sequential([
                    tf.keras.layers.LayerNormalization(input_shape=(input_shape,)),
                    tf.keras.layers.Dense(256, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.4),
                    tf.keras.layers.Dense(128, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.3),
                    tf.keras.layers.Dense(64, activation='swish'),
                    tf.keras.layers.BatchNormalization(),
                    tf.keras.layers.Dropout(0.2),
                    tf.keras.layers.Dense(32, activation='swish'),
                    tf.keras.layers.Dropout(0.1),
                    tf.keras.layers.Dense(n_classes, activation='softmax')
                ])
            
        except Exception as arch_error:
            logger.error(f"‚ùå Error creando arquitectura NN: {arch_error}")
            return None
        
        # Compilar modelo
        try:
            model.compile(
                optimizer=tf.keras.optimizers.AdamW(
                    learning_rate=0.001,
                    weight_decay=0.01
                ),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
        except Exception as compile_error:
            logger.error(f"‚ùå Error compilando NN: {compile_error}")
            return None
        
        # Callbacks
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=20,
                restore_best_weights=True,
                min_delta=0.001
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,
                min_lr=0.0001
            )
        ]
        
        # Entrenar
        try:
            batch_size = min(128, max(16, len(X_train) // 20))
            
            history = model.fit(
                X_train, y_train,
                epochs=100,
                batch_size=batch_size,
                validation_data=(X_test, y_test),
                callbacks=callbacks,
                verbose=0
            )
            
        except Exception as fit_error:
            logger.error(f"‚ùå Error entrenando NN: {fit_error}")
            return None
        
        # Evaluar
        try:
            _, accuracy = model.evaluate(X_test, y_test, verbose=0)
            
            # Evaluar se√±ales
            y_pred = model.predict(X_test, verbose=0)
            y_pred_classes = np.argmax(y_pred, axis=1)
            
            if n_classes > 2:
                signal_mask = y_test != 1
                if signal_mask.sum() > 0:
                    signal_accuracy = accuracy_score(y_test[signal_mask], y_pred_classes[signal_mask])
                else:
                    signal_accuracy = accuracy
            else:
                signal_accuracy = accuracy
            
            epochs_trained = len(history.history['loss'])
            
            logger.info(f"‚úÖ NN {trading_style} - Precisi√≥n: {accuracy:.2%} (se√±ales: {signal_accuracy:.2%}, √©pocas: {epochs_trained})")
            
            return {
                'model': model,
                'scaler': scaler,
                'accuracy': accuracy,
                'signal_accuracy': signal_accuracy,
                'history': history.history,
                'epochs_trained': epochs_trained,
                'n_classes': n_classes
            }
            
        except Exception as eval_error:
            logger.error(f"‚ùå Error evaluando NN: {eval_error}")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico en NN {trading_style}: {e}")
        return None

# AGREGAR ESTOS M√âTODOS A LA CLASE
UltraForexAI_V2.optimize_hyperparameters_for_style = optimize_hyperparameters_for_style
UltraForexAI_V2.get_default_params = get_default_params
UltraForexAI_V2.train_optimized_model = train_optimized_model
UltraForexAI_V2.train_deep_neural_network_for_style = train_deep_neural_network_for_style

print("‚úÖ Parte 5 recreada completamente - Sistema de optimizaci√≥n robusto")
print("üìù M√©todos agregados (con manejo de errores mejorado):")
print("   - optimize_hyperparameters_for_style() - Optimizaci√≥n con timeout y fallbacks")
print("   - get_default_params() - Par√°metros seguros por estilo")
print("   - train_optimized_model() - Entrenamiento con validaciones")
print("   - train_deep_neural_network_for_style() - NN robustas por estilo")
print("üõ°Ô∏è Caracter√≠sticas mejoradas:")
print("   - Manejo completo de errores en cada paso")
print("   - Timeouts para evitar cuelgues")
print("   - Par√°metros fallback seguros")
print("   - Validaciones de datos en cada etapa")
print("   - Logs detallados para debugging")

# ===== ULTRAFOREXAI V2 - PARTE 6: ENSEMBLE Y AUTO-ENTRENAMIENTO =====
print("üß† PARTE 6: Sistema de Ensemble Inteligente y Auto-Entrenamiento")
print("üéØ Objetivo: Combinar m√∫ltiples modelos y entrenar autom√°ticamente")
print("=" * 80)

import asyncio
import concurrent.futures
from datetime import datetime, timedelta
import threading
import time
import warnings
warnings.filterwarnings('ignore')

def create_ensemble_model_for_style(self, X, y, trading_style):
    """Crear ensemble inteligente espec√≠fico para cada estilo de trading"""
    
    try:
        from sklearn.metrics import accuracy_score, classification_report
        from sklearn.preprocessing import RobustScaler
        
        logger.info(f"üß† Creando ensemble para {trading_style}...")
        
        # Verificar datos m√≠nimos
        if len(X) < 20:  # Conservador como en el script exitoso
            logger.warning(f"‚ö†Ô∏è Datos insuficientes para ensemble: {len(X)}")
            return None
        
        style_config = self.get_style_config(trading_style)
        target_precision = style_config['target_precision']
        
        # Split temporal estricto para ensemble
        split_point = int(0.75 * len(X))
        X_train, X_holdout = X[:split_point], X[split_point:]
        y_train, y_holdout = y[:split_point], y[split_point:]
        
        # Segundo split para training/validation
        val_split = int(0.8 * len(X_train))
        X_train_inner, X_val = X_train[:val_split], X_train[val_split:]
        y_train_inner, y_val = y_train[:val_split], y_train[val_split:]
        
        logger.info(f"üìä Splits - Train: {len(X_train_inner)}, Val: {len(X_val)}, Holdout: {len(X_holdout)}")
        
        # Modelos base para el ensemble
        ensemble_models = {}
        model_performances = {}
        
        # 1. RANDOM FOREST
        try:
            logger.info("üå≤ Entrenando Random Forest...")
            rf_params = self.optimize_hyperparameters_for_style(X_train_inner, y_train_inner, 'RandomForest', trading_style)
            rf_result = self.train_optimized_model(X_train_inner, y_train_inner, 'RandomForest', rf_params)
            
            if rf_result:
                # Evaluar en validation
                rf_pred = rf_result['model'].predict(X_val)
                rf_accuracy = accuracy_score(y_val, rf_pred)
                
                ensemble_models['RandomForest'] = rf_result['model']
                model_performances['RandomForest'] = {
                    'accuracy': rf_accuracy,
                    'training_time': rf_result['training_time'],
                    'feature_importance': rf_result.get('feature_importance'),
                    'stability_score': self.calculate_stability_score(rf_result['model'], X_val, y_val)
                }
                logger.info(f"‚úÖ Random Forest: {rf_accuracy:.3f}")
            
        except Exception as e:
            logger.error(f"‚ùå Error Random Forest: {e}")
        
        # 2. XGBOOST
        try:
            logger.info("üöÄ Entrenando XGBoost...")
            xgb_params = self.optimize_hyperparameters_for_style(X_train_inner, y_train_inner, 'XGBoost', trading_style)
            xgb_result = self.train_optimized_model(X_train_inner, y_train_inner, 'XGBoost', xgb_params)
            
            if xgb_result:
                xgb_pred = xgb_result['model'].predict(X_val)
                xgb_accuracy = accuracy_score(y_val, xgb_pred)
                
                ensemble_models['XGBoost'] = xgb_result['model']
                model_performances['XGBoost'] = {
                    'accuracy': xgb_accuracy,
                    'training_time': xgb_result['training_time'],
                    'feature_importance': xgb_result.get('feature_importance'),
                    'stability_score': self.calculate_stability_score(xgb_result['model'], X_val, y_val)
                }
                logger.info(f"‚úÖ XGBoost: {xgb_accuracy:.3f}")
                
        except Exception as e:
            logger.error(f"‚ùå Error XGBoost: {e}")
        
        # 3. LIGHTGBM
        try:
            logger.info("‚ö° Entrenando LightGBM...")
            lgb_params = self.optimize_hyperparameters_for_style(X_train_inner, y_train_inner, 'LightGBM', trading_style)
            lgb_result = self.train_optimized_model(X_train_inner, y_train_inner, 'LightGBM', lgb_params)
            
            if lgb_result:
                lgb_pred = lgb_result['model'].predict(X_val)
                lgb_accuracy = accuracy_score(y_val, lgb_pred)
                
                ensemble_models['LightGBM'] = lgb_result['model']
                model_performances['LightGBM'] = {
                    'accuracy': lgb_accuracy,
                    'training_time': lgb_result['training_time'],
                    'feature_importance': lgb_result.get('feature_importance'),
                    'stability_score': self.calculate_stability_score(lgb_result['model'], X_val, y_val)
                }
                logger.info(f"‚úÖ LightGBM: {lgb_accuracy:.3f}")
                
        except Exception as e:
            logger.error(f"‚ùå Error LightGBM: {e}")
        
        # 4. NEURAL NETWORK
        try:
            logger.info("üß† Entrenando Red Neuronal...")
            nn_result = self.train_deep_neural_network_for_style(X_train_inner, y_train_inner, X_train_inner.shape[1], trading_style)
            
            if nn_result:
                # Evaluar NN en validation
                X_val_scaled = nn_result['scaler'].transform(X_val)
                nn_pred_proba = nn_result['model'].predict(X_val_scaled, verbose=0)
                nn_pred = np.argmax(nn_pred_proba, axis=1)
                nn_accuracy = accuracy_score(y_val, nn_pred)
                
                ensemble_models['NeuralNetwork'] = {
                    'model': nn_result['model'],
                    'scaler': nn_result['scaler']
                }
                model_performances['NeuralNetwork'] = {
                    'accuracy': nn_accuracy,
                    'training_time': 0,  # No disponible para NN
                    'feature_importance': None,
                    'stability_score': self.calculate_nn_stability_score(nn_result['model'], X_val_scaled, y_val)
                }
                logger.info(f"‚úÖ Neural Network: {nn_accuracy:.3f}")
                
        except Exception as e:
            logger.error(f"‚ùå Error Neural Network: {e}")
        
        # Verificar que tenemos modelos
        if len(ensemble_models) < 2:
            logger.error(f"‚ùå Ensemble fallido - Solo {len(ensemble_models)} modelos v√°lidos")
            return None
        
        # CALCULAR PESOS INTELIGENTES (como en el script exitoso)
        ensemble_weights = self.calculate_intelligent_weights(model_performances, trading_style)
        
        # CREAR META-MODELO PARA STACKING
        logger.info("üß† Creando meta-modelo para stacking...")
        
        # Obtener predicciones de modelos base en datos de validaci√≥n
        base_predictions = {}
        base_probabilities = {}
        
        for model_name, model in ensemble_models.items():
            try:
                if model_name == 'NeuralNetwork':
                    X_val_scaled = model['scaler'].transform(X_val)
                    proba = model['model'].predict(X_val_scaled, verbose=0)
                    pred = np.argmax(proba, axis=1)
                    base_probabilities[model_name] = proba
                else:
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba(X_val)
                        pred = model.predict(X_val)
                        base_probabilities[model_name] = proba
                    else:
                        pred = model.predict(X_val)
                        n_classes = len(np.unique(pred))
                        proba = np.eye(n_classes)[pred]
                        base_probabilities[model_name] = proba
                
                base_predictions[model_name] = pred
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error obteniendo predicciones de {model_name}: {e}")
        
        # Crear features para meta-modelo
        meta_features = []
        
        # Agregar predicciones hard de cada modelo base
        for model_name in ensemble_models.keys():
            if model_name in base_predictions:
                pred = base_predictions[model_name]
                # One-hot encoding de predicciones
                pred_onehot = np.eye(3)[pred]  # 3 clases: 0, 1, 2
                meta_features.append(pred_onehot)
        
        # Agregar probabilidades de cada modelo base
        for model_name in ensemble_models.keys():
            if model_name in base_probabilities:
                proba = base_probabilities[model_name]
                meta_features.append(proba)
        
        # Agregar features originales
        meta_features.append(X_val)
        
        # Concatenar todas las features
        meta_X = np.concatenate(meta_features, axis=1)
        
        # Crear y entrenar meta-modelo
        try:
            from sklearn.ensemble import RandomForestClassifier
            meta_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=42,
                n_jobs=-1
            )
            
            meta_model.fit(meta_X, y_val)
            logger.info("‚úÖ Meta-modelo entrenado exitosamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error entrenando meta-modelo: {e}")
            return None
        
        # CREAR ENSEMBLE STACKING
        ensemble = StackingEnsemble(ensemble_models, meta_model, trading_style)
        
        # EVALUAR ENSEMBLE EN HOLDOUT
        holdout_predictions = ensemble.predict(X_holdout)
        ensemble_accuracy = accuracy_score(y_holdout, holdout_predictions)
        
        # Evaluar solo se√±ales (no HOLD)
        if len(np.unique(y_holdout)) > 2:
            signal_mask = y_holdout != 1
            if signal_mask.sum() > 0:
                signal_accuracy = accuracy_score(y_holdout[signal_mask], holdout_predictions[signal_mask])
            else:
                signal_accuracy = ensemble_accuracy
        else:
            signal_accuracy = ensemble_accuracy
        
        logger.info(f"üéØ {trading_style} Ensemble Final:")
        logger.info(f"   üìä Precisi√≥n Total: {ensemble_accuracy:.3f}")
        logger.info(f"   üéØ Precisi√≥n Se√±ales: {signal_accuracy:.3f}")
        logger.info(f"   üî¢ Modelos: {len(ensemble_models)}")
        logger.info(f"   ‚öñÔ∏è Pesos: {ensemble_weights}")
        
        # Verificar si alcanza el target
        meets_target = signal_accuracy >= target_precision
        logger.info(f"   {'‚úÖ' if meets_target else '‚ö†Ô∏è'} Target {target_precision:.1%}: {'ALCANZADO' if meets_target else 'PENDIENTE'}")
        
        return {
            'ensemble': ensemble,
            'accuracy': ensemble_accuracy,
            'signal_accuracy': signal_accuracy,
            'meets_target': meets_target,
            'models_count': len(ensemble_models),
            'model_performances': model_performances,
            'weights': ensemble_weights,
            'holdout_size': len(X_holdout)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico creando ensemble {trading_style}: {e}")
        return None

def calculate_stability_score(self, model, X_test, y_test):
    """Calcular score de estabilidad del modelo"""
    try:
        from sklearn.metrics import accuracy_score
        
        # Dividir test set en chunks
        chunk_size = max(10, len(X_test) // 5)
        scores = []
        
        for i in range(0, len(X_test) - chunk_size, chunk_size):
            X_chunk = X_test[i:i + chunk_size]
            y_chunk = y_test[i:i + chunk_size]
            
            if len(np.unique(y_chunk)) > 1:  # Solo si hay m√∫ltiples clases
                pred_chunk = model.predict(X_chunk)
                score = accuracy_score(y_chunk, pred_chunk)
                scores.append(score)
        
        if len(scores) < 2:
            return 0.5
        
        # Estabilidad = 1 - coeficiente de variaci√≥n
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        
        if mean_score == 0:
            return 0.0
        
        cv = std_score / mean_score
        stability = max(0, 1 - cv)
        
        return min(1.0, stability)
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error calculando estabilidad: {e}")
        return 0.5

def calculate_nn_stability_score(self, model, X_test_scaled, y_test):
    """Calcular score de estabilidad para red neuronal"""
    try:
        from sklearn.metrics import accuracy_score
        
        chunk_size = max(10, len(X_test_scaled) // 5)
        scores = []
        
        for i in range(0, len(X_test_scaled) - chunk_size, chunk_size):
            X_chunk = X_test_scaled[i:i + chunk_size]
            y_chunk = y_test[i:i + chunk_size]
            
            if len(np.unique(y_chunk)) > 1:
                pred_proba = model.predict(X_chunk, verbose=0)
                pred_chunk = np.argmax(pred_proba, axis=1)
                score = accuracy_score(y_chunk, pred_chunk)
                scores.append(score)
        
        if len(scores) < 2:
            return 0.5
        
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        
        if mean_score == 0:
            return 0.0
        
        cv = std_score / mean_score
        stability = max(0, 1 - cv)
        
        return min(1.0, stability)
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error calculando estabilidad NN: {e}")
        return 0.5

def calculate_intelligent_weights(self, model_performances, trading_style):
    """Calcular pesos inteligentes basados en rendimiento y estilo"""
    
    if not model_performances:
        return {}
    
    logger.info(f"‚öñÔ∏è Calculando pesos inteligentes para {trading_style}...")
    
    weights = {}
    
    # Factores de peso por estilo
    style_factors = {
        'scalping': {
            'accuracy_weight': 0.5,
            'stability_weight': 0.4,
            'speed_weight': 0.1
        },
        'day_trading': {
            'accuracy_weight': 0.4,
            'stability_weight': 0.3,
            'speed_weight': 0.3
        },
        'swing_trading': {
            'accuracy_weight': 0.4,
            'stability_weight': 0.5,
            'speed_weight': 0.1
        },
        'position_trading': {
            'accuracy_weight': 0.3,
            'stability_weight': 0.6,
            'speed_weight': 0.1
        }
    }
    
    factors = style_factors.get(trading_style, style_factors['day_trading'])
    
    # Calcular scores combinados
    combined_scores = {}
    
    for model_name, performance in model_performances.items():
        accuracy = performance['accuracy']
        stability = performance['stability_score']
        
        # Speed score (invertir tiempo de entrenamiento)
        training_time = performance.get('training_time', 1)
        if training_time > 0:
            speed_score = 1 / (1 + training_time / 60)  # Normalizar por minutos
        else:
            speed_score = 0.8  # Default para NN
        
        # Score combinado
        combined_score = (
            accuracy * factors['accuracy_weight'] +
            stability * factors['stability_weight'] +
            speed_score * factors['speed_weight']
        )
        
        combined_scores[model_name] = combined_score
        
        logger.info(f"   {model_name}: Acc={accuracy:.3f}, Stab={stability:.3f}, Speed={speed_score:.3f} ‚Üí {combined_score:.3f}")
    
    # Convertir scores a pesos (softmax)
    scores_array = np.array(list(combined_scores.values()))
    
    # Aplicar softmax con temperatura para suavizar
    temperature = 2.0
    exp_scores = np.exp(scores_array / temperature)
    softmax_weights = exp_scores / np.sum(exp_scores)
    
    # Crear diccionario de pesos
    model_names = list(combined_scores.keys())
    for i, model_name in enumerate(model_names):
        weights[model_name] = float(softmax_weights[i])
    
    # Aplicar umbral m√≠nimo (evitar pesos muy peque√±os)
    min_weight = 0.05
    for model_name in weights.keys():
        if weights[model_name] < min_weight:
            weights[model_name] = min_weight
    
    # Renormalizar
    total_weight = sum(weights.values())
    weights = {k: v / total_weight for k, v in weights.items()}
    
    logger.info(f"‚öñÔ∏è Pesos finales: {weights}")
    
    return weights

class StackingEnsemble:
    """Ensemble con Stacking - Meta-modelo para combinar predicciones"""
    
    def __init__(self, base_models, meta_model, trading_style):
        self.base_models = base_models
        self.meta_model = meta_model
        self.trading_style = trading_style
        self.performance_history = []
        self.adaptation_rate = 0.1
        
        logger.info(f"üß† Stacking Ensemble {trading_style} creado con {len(base_models)} modelos base")
    
    def _get_base_predictions(self, X):
        """Obtener predicciones de todos los modelos base"""
        
        base_predictions = {}
        base_probabilities = {}
        
        for model_name, model in self.base_models.items():
            try:
                if model_name == 'NeuralNetwork':
                    # Caso especial para NN
                    X_scaled = model['scaler'].transform(X)
                    proba = model['model'].predict(X_scaled, verbose=0)
                    pred = np.argmax(proba, axis=1)
                    base_probabilities[model_name] = proba
                else:
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba(X)
                        pred = model.predict(X)
                        base_probabilities[model_name] = proba
                    else:
                        pred = model.predict(X)
                        # Convertir predicciones hard a probabilidades
                        n_classes = len(np.unique(pred))
                        proba = np.eye(n_classes)[pred]
                        base_probabilities[model_name] = proba
                
                base_predictions[model_name] = pred
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error predicci√≥n {model_name}: {e}")
                # Usar predicci√≥n por defecto (HOLD = 1)
                base_predictions[model_name] = np.ones(len(X), dtype=int)
                base_probabilities[model_name] = np.full((len(X), 3), 1/3)
        
        return base_predictions, base_probabilities
    
    def _create_meta_features(self, X, base_predictions, base_probabilities):
        """Crear features para el meta-modelo"""
        
        meta_features = []
        
        # Agregar predicciones hard de cada modelo base
        for model_name in self.base_models.keys():
            if model_name in base_predictions:
                pred = base_predictions[model_name]
                # One-hot encoding de predicciones
                pred_onehot = np.eye(3)[pred]  # 3 clases: 0, 1, 2
                meta_features.append(pred_onehot)
        
        # Agregar probabilidades de cada modelo base
        for model_name in self.base_models.keys():
            if model_name in base_probabilities:
                proba = base_probabilities[model_name]
                meta_features.append(proba)
        
        # Agregar features originales (importantes para stacking)
        meta_features.append(X)
        
        # Concatenar todas las features
        meta_X = np.concatenate(meta_features, axis=1)
        
        return meta_X
    
    def predict(self, X):
        """Predicci√≥n con stacking"""
        
        if len(X.shape) == 1:
            X = X.reshape(1, -1)
        
        try:
            # Obtener predicciones de modelos base
            base_predictions, base_probabilities = self._get_base_predictions(X)
            
            # Crear features para meta-modelo
            meta_X = self._create_meta_features(X, base_predictions, base_probabilities)
            
            # Predicci√≥n del meta-modelo
            if hasattr(self.meta_model, 'predict'):
                final_predictions = self.meta_model.predict(meta_X)
            else:
                # Fallback: votaci√≥n mayoritaria de modelos base
                all_predictions = list(base_predictions.values())
                final_predictions = []
                
                for i in range(len(X)):
                    votes = [pred[i] for pred in all_predictions if len(pred) > i]
                    if votes:
                        # Votaci√≥n mayoritaria
                        final_predictions.append(max(set(votes), key=votes.count))
                    else:
                        final_predictions.append(1)  # Default HOLD
                
                final_predictions = np.array(final_predictions)
            
            return final_predictions
            
        except Exception as e:
            logger.error(f"‚ùå Error en predicci√≥n stacking: {e}")
            # Fallback: votaci√≥n mayoritaria
            base_predictions, _ = self._get_base_predictions(X)
            all_predictions = list(base_predictions.values())
            
            if all_predictions:
                final_predictions = []
                for i in range(len(X)):
                    votes = [pred[i] for pred in all_predictions if len(pred) > i]
                    if votes:
                        final_predictions.append(max(set(votes), key=votes.count))
                    else:
                        final_predictions.append(1)
                return np.array(final_predictions)
            else:
                return np.ones(len(X), dtype=int)  # Default HOLD
    
    def predict_proba(self, X):
        """Predicci√≥n de probabilidades con stacking"""
        
        if len(X.shape) == 1:
            X = X.reshape(1, -1)
        
        try:
            # Obtener predicciones de modelos base
            base_predictions, base_probabilities = self._get_base_predictions(X)
            
            # Crear features para meta-modelo
            meta_X = self._create_meta_features(X, base_predictions, base_probabilities)
            
            # Predicci√≥n de probabilidades del meta-modelo
            if hasattr(self.meta_model, 'predict_proba'):
                ensemble_proba = self.meta_model.predict_proba(meta_X)
            else:
                # Fallback: promedio de probabilidades de modelos base
                all_probabilities = list(base_probabilities.values())
                if all_probabilities:
                    ensemble_proba = np.mean(all_probabilities, axis=0)
                else:
                    ensemble_proba = np.full((len(X), 3), 1/3)
            
            return ensemble_proba
            
        except Exception as e:
            logger.error(f"‚ùå Error en predict_proba stacking: {e}")
            # Fallback: probabilidades uniformes
            return np.full((len(X), 3), 1/3)
    
    def update_meta_model(self, X_val, y_val):
        """Actualizar meta-modelo con nuevos datos"""
        
        try:
            # Obtener predicciones de modelos base en datos de validaci√≥n
            base_predictions, base_probabilities = self._get_base_predictions(X_val)
            
            # Crear features para meta-modelo
            meta_X = self._create_meta_features(X_val, base_predictions, base_probabilities)
            
            # Re-entrenar meta-modelo
            if hasattr(self.meta_model, 'fit'):
                self.meta_model.fit(meta_X, y_val)
                logger.info(f"üîÑ Meta-modelo actualizado para {self.trading_style}")
            
        except Exception as e:
            logger.error(f"‚ùå Error actualizando meta-modelo: {e}")
    
    def get_model_importance(self):
        """Obtener importancia de cada modelo base"""
        
        try:
            if hasattr(self.meta_model, 'feature_importances_'):
                # Para Random Forest, XGBoost, etc.
                return self.meta_model.feature_importances_
            elif hasattr(self.meta_model, 'coef_'):
                # Para modelos lineales
                return np.abs(self.meta_model.coef_[0])
            else:
                # Fallback: pesos uniformes
                n_models = len(self.base_models)
                return np.ones(n_models) / n_models
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error obteniendo importancia: {e}")
            return np.ones(len(self.base_models)) / len(self.base_models)

def train_all_styles_parallel(self, symbol, max_workers=4):
    """Entrenar todos los estilos de trading en paralelo"""
    
    logger.info(f"üöÄ Entrenamiento paralelo para {symbol}...")
    
    trading_styles = ['scalping', 'day_trading', 'swing_trading', 'position_trading']
    results = {}
    
    def train_single_style(style):
        """Funci√≥n para entrenar un estilo espec√≠fico"""
        try:
            logger.info(f"üìä Iniciando entrenamiento {style} para {symbol}...")
            
            # Obtener datos espec√≠ficos para el estilo
            data = self.get_enhanced_data_multi(symbol, style)
            
            if data is None or len(data) < 30:  # REDUCIDO para permitir scalping
                logger.warning(f"‚ö†Ô∏è Datos insuficientes para {style}: {symbol}")
                return style, None
            
            # Validar calidad de datos
            is_valid, issues = self.validate_data_quality(data, style)
            if not is_valid:
                logger.warning(f"‚ö†Ô∏è Problemas de calidad en {style}: {issues}")
                # Continuar pero con precauci√≥n
            
            # Preparar features y target
            style_features = self.get_style_features(style)
            
            # Verificar que features existen
            available_features = [f for f in style_features if f in data.columns]
            if len(available_features) < len(style_features) * 0.7:  # Al menos 70% de features
                logger.warning(f"‚ö†Ô∏è Features faltantes en {style}: {len(available_features)}/{len(style_features)}")
            
            X = data[available_features].fillna(0)
            y = data['Direction'].fillna(1)  # Default HOLD
            
            # Verificar shapes
            if len(X) != len(y):
                logger.error(f"‚ùå Shape mismatch {style}: X={len(X)}, y={len(y)}")
                return style, None
            
            # Crear ensemble
            ensemble_result = self.create_ensemble_model_for_style(X.values, y.values, style)
            
            if ensemble_result:
                logger.info(f"‚úÖ {style} completado para {symbol}: {ensemble_result['accuracy']:.3f}")
            else:
                logger.error(f"‚ùå {style} fall√≥ para {symbol}")
            
            return style, ensemble_result
            
        except Exception as e:
            logger.error(f"‚ùå Error entrenando {style} para {symbol}: {e}")
            return style, None
    
    # Entrenar en paralelo
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Enviar todas las tareas
            future_to_style = {
                executor.submit(train_single_style, style): style 
                for style in trading_styles
            }
            
            # Recoger resultados con mejor manejo de interrupciones
            try:
                for future in concurrent.futures.as_completed(future_to_style, timeout=1800):  # 30 min timeout
                    try:
                        style, result = future.result()
                        results[style] = result
                        
                        if result:
                            logger.info(f"‚úÖ {style}: {result['accuracy']:.3f}")
                        else:
                            logger.warning(f"‚ö†Ô∏è {style}: FALL√ì")
                            
                    except Exception as e:
                        style = future_to_style[future]
                        logger.error(f"‚ùå Excepci√≥n en {style}: {e}")
                        results[style] = None
                        
            except KeyboardInterrupt:
                logger.warning("‚ö†Ô∏è Entrenamiento interrumpido por el usuario")
                # Cancelar tareas pendientes
                for future in future_to_style:
                    future.cancel()
                raise
            except concurrent.futures.TimeoutError:
                logger.error("‚ùå Timeout en entrenamiento paralelo")
                # Cancelar tareas pendientes
                for future in future_to_style:
                    future.cancel()
                raise
    
    except Exception as e:
        logger.error(f"‚ùå Error en entrenamiento paralelo: {e}")
    
    # Resumen final
    successful_styles = [style for style, result in results.items() if result is not None]
    
    logger.info(f"üéØ Resumen {symbol}:")
    logger.info(f"   ‚úÖ Exitosos: {len(successful_styles)}/{len(trading_styles)}")
    
    for style in successful_styles:
        result = results[style]
        meets_target = result['meets_target']
        logger.info(f"   {style}: {result['accuracy']:.3f} {'‚úÖ' if meets_target else '‚ö†Ô∏è'}")
    
    return results

def save_ensemble_models(self, symbol, style_results):
    """Guardar modelos ensemble en Drive si estamos en Colab"""
    
    try:
        # Detectar si estamos en Colab
        try:
            import google.colab
            is_colab = True
            drive_path = '/content/drive/MyDrive/aitraderx/models'
            os.makedirs(drive_path, exist_ok=True)
            logger.info(f"üöÄ Guardando en Google Drive: {drive_path}")
        except ImportError:
            is_colab = False
            drive_path = None
            logger.info("üíª Guardando localmente")
        
        # Crear directorio para el s√≠mbolo
        symbol_dir = os.path.join(self.models_dir, symbol)
        os.makedirs(symbol_dir, exist_ok=True)
        
        if is_colab:
            # Tambi√©n crear en Drive
            drive_symbol_dir = os.path.join(drive_path, symbol)
            os.makedirs(drive_symbol_dir, exist_ok=True)
        
        saved_models = {}
        
        for style, result in style_results.items():
            if result is None:
                continue
                
            try:
                # Guardar ensemble con formato correcto
                ensemble = result['ensemble']
                model_filename = f"{symbol}_{style}_ensemble.pkl"
                model_path = os.path.join(symbol_dir, model_filename)
                
                # Crear diccionario con toda la informaci√≥n necesaria
                ensemble_data = {
                    'ensemble': ensemble,
                    'accuracy': result['accuracy'],
                    'signal_accuracy': result['signal_accuracy'],
                    'models_count': result['models_count'],
                    'meets_target': result['meets_target'],
                    'model_performances': result.get('model_performances', {}),
                    'meta_model_type': result.get('meta_model_type', 'RandomForest'),
                    'holdout_size': result.get('holdout_size', 0),
                    'trading_style': style,
                    'symbol': symbol,
                    'saved_at': datetime.now().isoformat()
                }
                
                # Guardar localmente con formato correcto
                with open(model_path, 'wb') as f:
                    pickle.dump(ensemble_data, f)
                
                # Guardar en Drive si estamos en Colab
                if is_colab:
                    drive_model_path = os.path.join(drive_symbol_dir, model_filename)
                    with open(drive_model_path, 'wb') as f:
                        pickle.dump(ensemble_data, f)
                    logger.info(f"‚úÖ Guardado en Drive: {drive_model_path}")
                
                saved_models[style] = {
                    'local_path': model_path,
                    'drive_path': drive_model_path if is_colab else None,
                    'accuracy': result['accuracy'],
                    'signal_accuracy': result['signal_accuracy'],
                    'models_count': result['models_count']
                }
                
                logger.info(f"‚úÖ {style} guardado: {model_filename}")
                
            except Exception as e:
                logger.error(f"‚ùå Error guardando {style}: {e}")
        
        # Guardar metadatos
        metadata = {
            'symbol': symbol,
            'timestamp': datetime.now().isoformat(),
            'models': saved_models,
            'colab_backup': is_colab
        }
        
        metadata_path = os.path.join(symbol_dir, f"{symbol}_metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        if is_colab:
            drive_metadata_path = os.path.join(drive_symbol_dir, f"{symbol}_metadata.json")
            with open(drive_metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
        
        logger.info(f"üìä {len(saved_models)} modelos guardados para {symbol}")
        return saved_models
        
    except Exception as e:
        logger.error(f"‚ùå Error guardando modelos: {e}")
        return {}

def start_auto_training_system(self):
    """Iniciar sistema de auto-entrenamiento en thread separado"""
    
    if self.auto_training_active:
        logger.warning("‚ö†Ô∏è Auto-entrenamiento ya est√° activo")
        return
    
    logger.info("ü§ñ Iniciando sistema de auto-entrenamiento...")
    
    self.auto_training_active = True
    self.auto_training_thread = threading.Thread(
        target=self._auto_training_loop,
        daemon=True,
        name="AutoTrainingThread"
    )
    self.auto_training_thread.start()
    
    logger.info("‚úÖ Sistema de auto-entrenamiento iniciado")

def start_drift_monitoring_system(self):
    """Iniciar sistema de monitoreo de concept drift para todos los s√≠mbolos"""
    
    logger.info("üîç Iniciando sistema de monitoreo de concept drift...")
    
    # Iniciar monitoreo para cada s√≠mbolo y estilo
    for symbol in self.forex_pairs.values():
        for trading_style in self.trading_styles.keys():
            try:
                self.continuous_drift_monitoring(symbol, trading_style, check_interval_hours=6)
                logger.info(f"‚úÖ Monitoreo iniciado para {symbol} {trading_style}")
            except Exception as e:
                logger.error(f"‚ùå Error iniciando monitoreo para {symbol} {trading_style}: {e}")
    
    logger.info("‚úÖ Sistema de monitoreo de concept drift iniciado")

def stop_auto_training_system(self):
    """Detener sistema de auto-entrenamiento"""
    
    logger.info("üõë Deteniendo sistema de auto-entrenamiento...")
    
    self.auto_training_active = False
    
    if self.auto_training_thread and self.auto_training_thread.is_alive():
        self.auto_training_thread.join(timeout=30)
        if self.auto_training_thread.is_alive():
            logger.warning("‚ö†Ô∏è Thread de auto-entrenamiento no termin√≥ correctamente")
        else:
            logger.info("‚úÖ Auto-entrenamiento detenido correctamente")

def _auto_training_loop(self):
    """Loop principal del sistema de auto-entrenamiento"""
    
    logger.info("üîÑ Loop de auto-entrenamiento iniciado")
    
    last_training_time = {}
    retrain_interval = timedelta(hours=self.system_config['auto_retrain_hours'])
    
    while self.auto_training_active:
        try:
            current_time = datetime.now()
            
            for pair_name, pair_symbol in self.forex_pairs.items():
                
                # Verificar si necesita re-entrenamiento
                last_trained = last_training_time.get(pair_name)
                
                if last_trained is None or (current_time - last_trained) >= retrain_interval:
                    
                    logger.info(f"üîÑ Auto-entrenamiento programado para {pair_name}...")
                    
                    try:
                        # Entrenar todos los estilos
                        results = self.train_all_styles_parallel(pair_symbol)
                        
                        # Guardar modelos
                        self.save_ensemble_models(pair_symbol, results)
                        
                        # Actualizar tiempo de entrenamiento
                        last_training_time[pair_name] = current_time
                        
                        # Log del entrenamiento
                        training_log = {
                            'timestamp': current_time.isoformat(),
                            'symbol': pair_symbol,
                            'pair_name': pair_name,
                            'results_summary': {
                                style: {
                                    'success': result is not None,
                                    'accuracy': result['accuracy'] if result else None,
                                    'meets_target': result['meets_target'] if result else False
                                }
                                for style, result in results.items()
                            }
                        }
                        
                        self.system_results['auto_training_log'].append(training_log)
                        
                        logger.info(f"‚úÖ Auto-entrenamiento completado para {pair_name}")
                        
                    except Exception as e:
                        logger.error(f"‚ùå Error en auto-entrenamiento {pair_name}: {e}")
                        
                        # Log del error
                        error_log = {
                            'timestamp': current_time.isoformat(),
                            'symbol': pair_symbol,
                            'pair_name': pair_name,
                            'error': str(e),
                            'type': 'auto_training_error'
                        }
                        
                        self.system_results['auto_training_log'].append(error_log)
            
            # Esperar antes del pr√≥ximo ciclo
            logger.info(f"üí§ Auto-entrenamiento dormir√° por {self.system_config['auto_retrain_hours']} horas...")
            
            # Dormir en chunks para poder interrumpir
            sleep_time = self.system_config['auto_retrain_hours'] * 3600  # Convertir a segundos
            chunk_size = 300  # 5 minutos por chunk
            
            for _ in range(int(sleep_time // chunk_size)):
                if not self.auto_training_active:
                    break
                time.sleep(chunk_size)
            
            # Dormir el tiempo restante
            remaining_sleep = sleep_time % chunk_size
            if remaining_sleep > 0 and self.auto_training_active:
                time.sleep(remaining_sleep)
                
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en loop auto-entrenamiento: {e}")
            time.sleep(300)  # Esperar 5 minutos antes de reintentar
    
    logger.info("üèÅ Loop de auto-entrenamiento terminado")

def get_ensemble_prediction(self, symbol, trading_style, current_data):
    """Obtener predicci√≥n del ensemble para s√≠mbolo y estilo espec√≠fico"""
    
    try:
        # Cargar modelo ensemble
        model_file = self.models_dir / symbol / f"{trading_style}_ensemble.pkl"
        
        if not model_file.exists():
            logger.warning(f"‚ö†Ô∏è Modelo {trading_style} no encontrado para {symbol}")
            return None
        
        # Cargar ensemble
        with open(model_file, 'rb') as f:
            ensemble_data = pickle.load(f)
        ensemble = ensemble_data['ensemble']
        
        # Preparar datos actuales
        style_features = self.get_style_features(trading_style)
        
        # Verificar que tenemos todos los features necesarios
        available_features = [f for f in style_features if f in current_data.columns]
        
        if len(available_features) < len(style_features) * 0.7:
            logger.warning(f"‚ö†Ô∏è Features insuficientes para predicci√≥n {trading_style}")
            return None
        
        X_current = current_data[available_features].fillna(0).values
        
        if len(X_current.shape) == 1:
            X_current = X_current.reshape(1, -1)
        
        # Obtener predicci√≥n
        prediction = ensemble.predict(X_current)
        prediction_proba = ensemble.predict_proba(X_current)
        
        # Mapear predicci√≥n a se√±al
        signal_map = {0: 'SELL', 1: 'HOLD', 2: 'BUY'}
        signal = signal_map.get(prediction[0], 'HOLD')
        
        # Calcular confianza
        confidence = float(np.max(prediction_proba[0]))
        
        return {
            'signal': signal,
            'confidence': confidence,
            'prediction_numeric': int(prediction[0]),
            'probabilities': {
                'SELL': float(prediction_proba[0][0]),
                'HOLD': float(prediction_proba[0][1]),
                'BUY': float(prediction_proba[0][2])
            },
            'model_info': {
                'trading_style': trading_style,
                'symbol': symbol,
                'models_count': ensemble_data['models_count'],
                'trained_accuracy': ensemble_data['accuracy'],
                'meets_target': ensemble_data['meets_target']
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo predicci√≥n {trading_style} para {symbol}: {e}")
        return None

def get_multi_style_consensus(self, symbol, current_data):
    """Obtener consenso entre m√∫ltiples estilos de trading"""
    
    logger.info(f"üéØ Obteniendo consenso multi-estilo para {symbol}...")
    
    styles = ['scalping', 'day_trading', 'swing_trading', 'position_trading']
    predictions = {}
    
    # Obtener predicciones de todos los estilos
    for style in styles:
        pred = self.get_ensemble_prediction(symbol, style, current_data)
        if pred:
            predictions[style] = pred
    
    if not predictions:
        logger.warning(f"‚ö†Ô∏è No hay predicciones disponibles para {symbol}")
        return None
    
    # Analizar consenso
    signals = [pred['signal'] for pred in predictions.values()]
    confidences = [pred['confidence'] for pred in predictions.values()]
    
    # Contar votos por se√±al
    signal_votes = {}
    weighted_votes = {}
    
    # Pesos por estilo (m√°s peso a estilos de mayor frecuencia)
    style_weights = {
        'scalping': 0.4,
        'day_trading': 0.3,
        'swing_trading': 0.2,
        'position_trading': 0.1
    }
    
    for style, pred in predictions.items():
        signal = pred['signal']
        confidence = pred['confidence']
        weight = style_weights.get(style, 0.25)
        
        # Votos simples
        if signal not in signal_votes:
            signal_votes[signal] = 0
        signal_votes[signal] += 1
        
        # Votos ponderados por confianza y peso de estilo
        weighted_score = confidence * weight
        if signal not in weighted_votes:
            weighted_votes[signal] = 0
        weighted_votes[signal] += weighted_score
    
    # Determinar se√±al final
    consensus_signal = max(weighted_votes.keys(), key=lambda k: weighted_votes[k])
    simple_consensus = max(signal_votes.keys(), key=lambda k: signal_votes[k])
    
    # Calcular nivel de consenso
    total_predictions = len(predictions)
    consensus_strength = signal_votes[consensus_signal] / total_predictions
    
    # Calcular confianza promedio
    avg_confidence = np.mean(confidences)
    
    # Determinar calidad del consenso
    consensus_quality = "HIGH" if consensus_strength >= 0.75 else "MEDIUM" if consensus_strength >= 0.5 else "LOW"
    
    logger.info(f"üéØ Consenso {symbol}: {consensus_signal} ({consensus_quality}, {consensus_strength:.1%})")
    
    return {
        'consensus_signal': consensus_signal,
        'simple_consensus': simple_consensus,
        'consensus_strength': consensus_strength,
        'consensus_quality': consensus_quality,
        'avg_confidence': avg_confidence,
        'individual_predictions': predictions,
        'signal_votes': signal_votes,
        'weighted_votes': weighted_votes,
        'total_styles': total_predictions
    }

def run_complete_training_pipeline(self, symbols=None):
    """Ejecutar pipeline completo de entrenamiento para m√∫ltiples s√≠mbolos"""
    
    if symbols is None:
        symbols = list(self.forex_pairs.values())
    
    logger.info(f"üöÄ Iniciando pipeline completo para {len(symbols)} s√≠mbolos...")
    logger.info(f"üìã S√≠mbolos: {symbols}")
    
    pipeline_start = datetime.now()
    pipeline_results = {}
    
    for i, symbol in enumerate(symbols, 1):
        logger.info(f"{'='*60}")
        logger.info(f"üìä Procesando s√≠mbolo {i}/{len(symbols)}: {symbol}")
        logger.info(f"{'='*60}")
        
        symbol_start = time.time()
        
        try:
            # Entrenar todos los estilos en paralelo
            style_results = self.train_all_styles_parallel(symbol, max_workers=2)
            
            # Guardar modelos
            self.save_ensemble_models(symbol, style_results)
            
            # Calcular m√©tricas de resumen
            successful_styles = [s for s, r in style_results.items() if r is not None]
            target_met_styles = [s for s, r in style_results.items() if r and r['meets_target']]
            
            avg_accuracy = np.mean([r['accuracy'] for r in style_results.values() if r])
            avg_signal_accuracy = np.mean([r['signal_accuracy'] for r in style_results.values() if r])
            
            symbol_time = time.time() - symbol_start
            
            pipeline_results[symbol] = {
                'successful_styles': successful_styles,
                'target_met_styles': target_met_styles,
                'avg_accuracy': avg_accuracy,
                'avg_signal_accuracy': avg_signal_accuracy,
                'training_time_seconds': symbol_time,
                'style_results': style_results
            }
            
            logger.info(f"‚úÖ {symbol} completado en {symbol_time:.1f}s:")
            logger.info(f"   üìä Estilos exitosos: {len(successful_styles)}/4")
            logger.info(f"   üéØ Targets alcanzados: {len(target_met_styles)}")
            logger.info(f"   üìà Precisi√≥n promedio: {avg_accuracy:.3f}")
            logger.info(f"   üéØ Precisi√≥n se√±ales: {avg_signal_accuracy:.3f}")
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando {symbol}: {e}")
            pipeline_results[symbol] = {
                'error': str(e),
                'successful_styles': [],
                'target_met_styles': [],
                'avg_accuracy': 0,
                'avg_signal_accuracy': 0,
                'training_time_seconds': time.time() - symbol_start
            }
    
    pipeline_end = datetime.now()
    total_time = (pipeline_end - pipeline_start).total_seconds()
    
    # Generar reporte final
    logger.info(f"{'='*80}")
    logger.info(f"üéØ REPORTE FINAL DEL PIPELINE")
    logger.info(f"{'='*80}")
    
    total_successful = sum(len(r['successful_styles']) for r in pipeline_results.values())
    total_possible = len(symbols) * 4  # 4 estilos por s√≠mbolo
    
    total_targets_met = sum(len(r['target_met_styles']) for r in pipeline_results.values())
    
    overall_accuracy = np.mean([r['avg_accuracy'] for r in pipeline_results.values() if r['avg_accuracy'] > 0])
    overall_signal_accuracy = np.mean([r['avg_signal_accuracy'] for r in pipeline_results.values() if r['avg_signal_accuracy'] > 0])
    
    logger.info(f"‚è±Ô∏è  Tiempo total: {total_time:.1f} segundos ({total_time/60:.1f} minutos)")
    logger.info(f"üìä Modelos entrenados: {total_successful}/{total_possible} ({total_successful/total_possible:.1%})")
    logger.info(f"üéØ Targets alcanzados: {total_targets_met}")
    logger.info(f"üìà Precisi√≥n promedio general: {overall_accuracy:.3f}")
    logger.info(f"üéØ Precisi√≥n se√±ales general: {overall_signal_accuracy:.3f}")
    
    # Detalles por s√≠mbolo
    logger.info(f"\nüìã DETALLES POR S√çMBOLO:")
    for symbol, result in pipeline_results.items():
        if 'error' in result:
            logger.info(f"‚ùå {symbol}: ERROR - {result['error']}")
        else:
            logger.info(f"‚úÖ {symbol}: {len(result['successful_styles'])}/4 estilos, {len(result['target_met_styles'])} targets")
    
    # Guardar resultados del pipeline
    self.system_results['pipeline_results'] = pipeline_results
    self.system_results['pipeline_completed_at'] = pipeline_end.isoformat()
    
    # Guardar reporte en archivo
    report_file = self.logs_dir / f"pipeline_report_{pipeline_end.strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w') as f:
        json.dump({
            'pipeline_results': pipeline_results,
            'summary': {
                'total_time_seconds': total_time,
                'models_trained': total_successful,
                'targets_met': total_targets_met,
                'overall_accuracy': overall_accuracy,
                'overall_signal_accuracy': overall_signal_accuracy
            }
        }, f, indent=2, default=str)
    
    logger.info(f"üìÑ Reporte guardado en: {report_file}")
    logger.info(f"{'='*80}")
    
    return pipeline_results

# AGREGAR TODOS LOS M√âTODOS A LA CLASE
UltraForexAI_V2.create_ensemble_model_for_style = create_ensemble_model_for_style
UltraForexAI_V2.calculate_stability_score = calculate_stability_score
UltraForexAI_V2.calculate_nn_stability_score = calculate_nn_stability_score
UltraForexAI_V2.calculate_intelligent_weights = calculate_intelligent_weights
UltraForexAI_V2.train_all_styles_parallel = train_all_styles_parallel
UltraForexAI_V2.save_ensemble_models = save_ensemble_models
UltraForexAI_V2.start_auto_training_system = start_auto_training_system
UltraForexAI_V2.start_drift_monitoring_system = start_drift_monitoring_system
UltraForexAI_V2.stop_auto_training_system = stop_auto_training_system
UltraForexAI_V2._auto_training_loop = _auto_training_loop
UltraForexAI_V2.get_ensemble_prediction = get_ensemble_prediction
UltraForexAI_V2.get_multi_style_consensus = get_multi_style_consensus
UltraForexAI_V2.run_complete_training_pipeline = run_complete_training_pipeline

# ===== FUNCIONES DE TESTING PARA YAHOO FINANCE =====

def test_single_symbol(symbol):
    """Prueba r√°pida de un s√≠mbolo individual"""
    logger.info(f"üß™ PROBANDO: {symbol}")
    
    try:
        data = fixed_yf_download(symbol, period='1mo')
        if not data.empty:
            logger.info(f"‚úÖ √âXITO: {len(data)} registros obtenidos")
            logger.info(f"üìÖ Rango: {data.index[0].date()} to {data.index[-1].date()}")
            logger.info(f"üí∞ √öltimo precio: {data['Close'].iloc[-1]:.4f}")
            logger.info("\nüìä √öltimos 3 registros:")
            logger.info(data.tail(3)[['Open', 'High', 'Low', 'Close']])
            return True
        else:
            logger.error("‚ùå FALLO: Datos vac√≠os")
            return False
    except Exception as e:
        logger.error(f"‚ùå FALLO: {e}")
        return False

def test_all_problematic_symbols():
    """Prueba todos los s√≠mbolos que estaban fallando"""
    symbols = ['EURUSD=X', 'USDJPY=X', 'GBPUSD=X', 'AUDUSD=X', 'USDCAD=X']
    
    logger.info("üöÄ PROBANDO TODOS LOS S√çMBOLOS PROBLEM√ÅTICOS")
    logger.info("=" * 50)
    
    results = {}
    for symbol in symbols:
        logger.info(f"\n{'-' * 30}")
        success = test_single_symbol(symbol)
        results[symbol] = success
        time.sleep(5)  # Pausa entre pruebas
    
    logger.info(f"\n{'=' * 50}")
    logger.info("üìã RESUMEN DE PRUEBAS:")
    successful = sum(results.values())
    for symbol, success in results.items():
        status = "‚úÖ" if success else "‚ùå"
        logger.info(f"  {status} {symbol}")
    
    logger.info(f"\nüéØ Total exitosos: {successful}/{len(symbols)}")
    return results

def train_models_with_fixes(symbols, trading_styles):
    """
    Versi√≥n mejorada del loop de entrenamiento con manejo robusto
    """
    results = {
        'successful': 0,
        'failed': 0,
        'details': {}
    }
    
    logger.info(f"üöÄ ENTRENAMIENTO MEJORADO - {len(symbols)} S√çMBOLOS")
    logger.info(f"üìä S√≠mbolos: {symbols}")
    logger.info(f"üéØ Estilos: {trading_styles}")
    logger.info("=" * 60)
    
    for i, symbol in enumerate(symbols, 1):
        logger.info(f"\nüíª PROCESANDO {i}/{len(symbols)}: {symbol}")
        logger.info(f"‚è∞ Inicio: {time.strftime('%H:%M:%S')}")
        logger.info("=" * 50)
        
        symbol_results = {}
        symbol_successful = 0
        
        for style in trading_styles:
            try:
                logger.info(f"\nüéØ Estilo: {style}")
                
                # Usar la funci√≥n mejorada
                data = get_market_data_fixed(symbol, period='1mo', trading_style=style)
                
                logger.info(f"‚úÖ Datos obtenidos: {len(data)} registros")
                logger.info(f"üìÖ Rango: {data.index[0].date()} to {data.index[-1].date()}")
                
                # Aqu√≠ va tu l√≥gica de entrenamiento actual
                # model_result = train_model(data, style)
                # symbol_results[style] = model_result
                
                symbol_results[style] = {
                    'status': 'SUCCESS',
                    'rows': len(data),
                    'date_range': f"{data.index[0].date()} to {data.index[-1].date()}"
                }
                
                symbol_successful += 1
                logger.info(f"‚úÖ {style}: COMPLETADO")
                
            except Exception as e:
                logger.error(f"‚ùå Error en {style}: {e}")
                symbol_results[style] = {
                    'status': 'FAILED',
                    'error': str(e)
                }
                logger.info(f"‚ö†Ô∏è {style}: FALL√ì")
        
        # Guardar resultados del s√≠mbolo
        results['details'][symbol] = {
            'successful_styles': symbol_successful,
            'total_styles': len(trading_styles),
            'success_rate': (symbol_successful / len(trading_styles)) * 100,
            'results': symbol_results
        }
        
        if symbol_successful > 0:
            results['successful'] += 1
        else:
            results['failed'] += 1
        
        completion_time = time.strftime('%H:%M:%S')
        logger.info(f"‚úÖ {symbol} completado: {symbol_successful}/{len(trading_styles)} estilos exitosos")
        logger.info(f"‚è∞ Fin: {completion_time}")
        
        # Pausa entre s√≠mbolos para evitar rate limiting
        if i < len(symbols):
            logger.info(f"‚è∏Ô∏è Pausa 15 segundos antes del siguiente s√≠mbolo...")
            time.sleep(15)
    
    # Resumen final
    logger.info(f"\n{'=' * 60}")
    logger.info("üéâ ENTRENAMIENTO COMPLETADO")
    logger.info("=" * 60)
    logger.info(f"‚úÖ S√≠mbolos exitosos: {results['successful']}/{len(symbols)}")
    logger.info(f"‚ùå S√≠mbolos fallidos: {results['failed']}/{len(symbols)}")
    
    for symbol, details in results['details'].items():
        success_rate = details['success_rate']
        logger.info(f"  üìä {symbol}: {details['successful_styles']}/{details['total_styles']} estilos ({success_rate:.1f}%)")
    
    return results

# ===== FUNCI√ìN DE PRUEBA DE INTEGRACI√ìN =====

def test_integration():
    """Prueba r√°pida de la integraci√≥n"""
    print("\nüß™ PROBANDO INTEGRACI√ìN...")
    
    try:
        # Crear instancia
        ai_system = UltraForexAI_V2()
        
        # Probar obtenci√≥n de datos con el m√©todo corregido
        test_data = ai_system.get_enhanced_data_multi('EURUSD=X', 'day_trading')
        
        if test_data is not None and len(test_data) > 50:
            print(f"‚úÖ INTEGRACI√ìN EXITOSA: {len(test_data)} registros obtenidos")
            print(f"   Features disponibles: {len(test_data.columns)} columnas")
            print(f"   Target creado: {'Direction' in test_data.columns}")
            return True
        else:
            print("‚ùå INTEGRACI√ìN FALL√ì: Datos insuficientes")
            return False
            
    except Exception as e:
        print(f"‚ùå INTEGRACI√ìN FALL√ì: {e}")
        return False

print("‚úÖ Parte 6 completada - Sistema de Ensemble y Auto-Entrenamiento")
print("üìù M√©todos agregados:")
print("   - create_ensemble_model_for_style() - Ensemble inteligente por estilo")
print("   - calculate_intelligent_weights() - Pesos adaptativos")
print("   - train_all_styles_parallel() - Entrenamiento paralelo")
print("   - IntelligentEnsemble class - Ensemble con meta-learning")
print("   - Auto-training system - Sistema autom√°tico 24/7")
print("   - get_multi_style_consensus() - Consenso entre estilos")
print("   - run_complete_training_pipeline() - Pipeline completo")

print("\nüöÄ EJEMPLO DE USO:")
print("""
# Crear instancia del sistema
ai_system = UltraForexAI_V2()

# Instalar dependencias si es necesario
ai_system.install_missing_dependencies()

# Ejecutar pipeline completo de entrenamiento
results = ai_system.run_complete_training_pipeline()

# Iniciar sistema de auto-entrenamiento
ai_system.start_auto_training_system()

# Iniciar monitoreo de concept drift
ai_system.start_drift_monitoring_system()

# Obtener predicci√≥n para s√≠mbolo espec√≠fico
current_data = ai_system.get_enhanced_data_multi('EURUSD=X', 'day_trading')
if current_data is not None:
    # Consenso multi-estilo
    consensus = ai_system.get_multi_style_consensus('EURUSD=X', current_data.tail(1))
    print(f"Consenso: {consensus['consensus_signal']} (Confianza: {consensus['avg_confidence']:.2%})")
    
    # Predicci√≥n espec√≠fica por estilo
    day_pred = ai_system.get_ensemble_prediction('EURUSD=X', 'day_trading', current_data.tail(1))
    if day_pred:
        print(f"Day Trading: {day_pred['signal']} (Confianza: {day_pred['confidence']:.2%})")

# Detener auto-entrenamiento cuando termine
# ai_system.stop_auto_training_system()
""")

print("\nüéØ CARACTER√çSTICAS PRINCIPALES:")
print("   ‚úÖ Ensemble inteligente con 4 tipos de modelos por estilo")
print("   ‚úÖ Pesos adaptativos basados en rendimiento y estabilidad")  
print("   ‚úÖ Entrenamiento paralelo para m√°xima eficiencia")
print("   ‚úÖ Auto-entrenamiento 24/7 con intervalos configurables")
print("   ‚úÖ Consenso multi-estilo para decisiones robustas") 
print("   ‚úÖ Guardado autom√°tico de modelos y metadata")
print("   ‚úÖ Sistema de logging completo y reportes detallados")
print("   ‚úÖ Manejo robusto de errores en cada etapa")
print("   ‚úÖ Detecci√≥n de concept drift en tiempo real")
print("   ‚úÖ Re-entrenamiento adaptativo autom√°tico")
print("   ‚úÖ Monitoreo continuo de rendimiento de modelos")

print("\n‚ö° OPTIMIZACIONES IMPLEMENTADAS:")
print("   üî• Procesamiento paralelo con ThreadPoolExecutor")
print("   üî• Cache inteligente de modelos entrenados")
print("   üî• Validaci√≥n exhaustiva de datos y modelos")
print("   üî• Timeouts para evitar cuelgues indefinidos")
print("   üî• Fallbacks seguros en caso de errores")
print("   üî• M√©tricas de estabilidad y adaptaci√≥n de pesos")

print("\nüèÜ TARGETS DE RENDIMIENTO:")
print("   üéØ Scalping: 85%+ precisi√≥n (ultra-conservador)")
print("   üéØ Day Trading: 75%+ precisi√≥n (balanceado)")
print("   üéØ Swing Trading: 70%+ precisi√≥n (tendencias)")
print("   üéØ Position Trading: 65%+ precisi√≥n (macro)")

print("\n" + "="*80)
print("üéâ ¬°ULTRAFOREXAI V2 CON MEJORAS APLICADAS!")
print("="*80)

print("\nüìä MEJORAS APLICADAS DEL ARCHIVO EXITOSO:")
print("   ‚úÖ Per√≠odos de datos m√°s conservadores (1mo, 3mo, 1y, 5y)")
print("   ‚úÖ Umbrales m√≠nimos reducidos para mayor estabilidad")
print("   ‚úÖ Distribuci√≥n de targets m√°s equilibrada (25% BUY, 25% SELL, 50% HOLD)")
print("   ‚úÖ Trials de optimizaci√≥n reducidos (15 para scalping, 30 para otros)")
print("   ‚úÖ Configuraci√≥n m√°s robusta y estable")

print("\nüîç NUEVA FUNCIONALIDAD DE CONCEPT DRIFT:")
print("   ‚úÖ detect_concept_drift() - Detecci√≥n en tiempo real")
print("   ‚úÖ evaluate_model_performance() - Evaluaci√≥n de rendimiento")
print("   ‚úÖ adaptive_model_retraining() - Re-entrenamiento adaptativo")
print("   ‚úÖ continuous_drift_monitoring() - Monitoreo continuo")
print("   ‚úÖ start_drift_monitoring_system() - Sistema autom√°tico")

print("\nüß™ FUNCIONES DE PRUEBA AGREGADAS:")
print("   ‚úÖ test_concept_drift_detection() - Prueba completa de concept drift")
print("   ‚úÖ run_quick_test() - Ahora incluye prueba de concept drift")

print("\nüéØ BENEFICIOS ESPERADOS:")
print("   üìà Mayor estabilidad en el entrenamiento")
print("   üìä Mejor distribuci√≥n de targets")
print("   ‚ö° Entrenamiento m√°s r√°pido")
print("   üîÑ Detecci√≥n autom√°tica de concept drift")
print("   üöÄ Re-entrenamiento adaptativo autom√°tico")
print("   üìâ Menor tasa de fallos")
print("   üéØ Mejor precisi√≥n general")

print("\nüîß FUNCIONES DE TESTING DISPONIBLES:")
print("   - test_single_symbol(symbol) - Probar un s√≠mbolo espec√≠fico")
print("   - test_all_problematic_symbols() - Probar todos los s√≠mbolos")
print("   - train_models_with_fixes(symbols, styles) - Entrenamiento robusto")

print("\nüöÄ EJEMPLO DE USO CON PARCHE:")
print("""
# Crear instancia del sistema
ai_system = UltraForexAI_V2()

# Probar s√≠mbolos individuales
test_single_symbol('EURUSD=X')

# Probar todos los s√≠mbolos problem√°ticos
test_all_problematic_symbols()

# Entrenamiento robusto
forex_symbols = ['EURUSD=X', 'USDJPY=X', 'GBPUSD=X', 'AUDUSD=X', 'USDCAD=X']
trading_styles = ['day_trading', 'scalping', 'swing_trading', 'position_trading']
results = train_models_with_fixes(forex_symbols, trading_styles)

# Pipeline completo con manejo robusto
results = ai_system.run_complete_training_pipeline()
""")

print("\nüõ°Ô∏è CARACTER√çSTICAS DEL PARCHE:")
print("   ‚úÖ Rate limiting autom√°tico (4 llamadas/minuto)")
print("   ‚úÖ M√∫ltiples m√©todos de descarga")
print("   ‚úÖ S√≠mbolos alternativos para cada par")
print("   ‚úÖ Reintentos autom√°ticos con pausas")
print("   ‚úÖ Manejo robusto de errores")
print("   ‚úÖ Logging detallado de intentos")
print("   ‚úÖ Fallbacks seguros")

print("\nüîß INTEGRACI√ìN COMPLETADA:")
print("   ‚úÖ M√©todo get_enhanced_data_multi reemplazado con versi√≥n corregida")
print("   ‚úÖ Fix de Yahoo Finance integrado")
print("   ‚úÖ Per√≠odos ajustados para mejor estabilidad")
print("   ‚úÖ S√≠mbolos alternativos configurados")
print("   ‚úÖ Manejo robusto de errores a√±adido")

print("\nüß™ FUNCI√ìN DE PRUEBA DISPONIBLE:")
print("   - test_integration() - Prueba r√°pida de la integraci√≥n")

print("\n" + "="*80)

print("\n" + "="*80)

# ===== EJECUCI√ìN AUTOM√ÅTICA DEL SISTEMA =====
print("üöÄ INICIANDO EJECUCI√ìN AUTOM√ÅTICA DEL SISTEMA")
print("=" * 80)

def run_automatic_training():
    """Ejecutar entrenamiento autom√°tico del sistema"""
    
    print("üéØ EJECUTANDO ENTRENAMIENTO AUTOM√ÅTICO ULTRAFOREXAI V2")
    print("üìä Pares: EURUSD, USDJPY, GBPUSD, AUDUSD, USDCAD")
    print("üéØ Estilos: Scalping, Day Trading, Swing Trading, Position Trading")
    print("‚è±Ô∏è Tiempo estimado: 20-30 minutos")
    print("=" * 60)
    
    start_time = datetime.now()
    
    try:
        # Crear instancia del sistema
        print("üîß Inicializando UltraForexAI V2...")
        ai_system = UltraForexAI_V2()
        
        # Verificar estructura de modelos
        print("üîç Verificando estructura de modelos...")
        ai_system.verify_model_structure()
        ai_system.clean_old_models()
        
        # Instalar dependencias si es necesario
        print("üì¶ Verificando dependencias...")
        ai_system.install_missing_dependencies()
        
        # Probar integraci√≥n
        print("üß™ Probando integraci√≥n del sistema...")
        if test_integration():
            print("‚úÖ Integraci√≥n exitosa - Continuando con entrenamiento")
        else:
            print("‚ö†Ô∏è Problemas de integraci√≥n - Continuando de todas formas")
        
        # Ejecutar pipeline completo
        print("üöÄ Iniciando entrenamiento completo...")
        results = ai_system.run_complete_training_pipeline()
        
        # Iniciar monitoreo de concept drift
        print("üîç Iniciando monitoreo de concept drift...")
        ai_system.start_drift_monitoring_system()
        
        # Mostrar resultados
        if results:
            print("\n" + "="*60)
            print("üìä RESULTADOS DEL ENTRENAMIENTO")
            print("="*60)
            
            total_symbols = len(results)
            successful_symbols = sum(1 for r in results.values() if 'error' not in r)
            
            print(f"üìà S√≠mbolos procesados: {total_symbols}")
            print(f"‚úÖ S√≠mbolos exitosos: {successful_symbols}")
            print(f"‚ùå S√≠mbolos fallidos: {total_symbols - successful_symbols}")
            
            print("\nüìã DETALLES POR S√çMBOLO:")
            for symbol, result in results.items():
                if 'error' in result:
                    print(f"‚ùå {symbol}: ERROR - {result['error']}")
                else:
                    successful_styles = len(result.get('successful_styles', []))
                    target_met = len(result.get('target_met_styles', []))
                    avg_acc = result.get('avg_accuracy', 0)
                    avg_signal = result.get('avg_signal_accuracy', 0)
                    
                    print(f"‚úÖ {symbol}:")
                    print(f"   üìä Estilos exitosos: {successful_styles}/4")
                    print(f"   üéØ Targets alcanzados: {target_met}")
                    print(f"   üìà Precisi√≥n promedio: {avg_acc:.3f}")
                    print(f"   üéØ Precisi√≥n se√±ales: {avg_signal:.3f}")
            
            # Tiempo total
            total_time = (datetime.now() - start_time).total_seconds()
            print(f"\n‚è±Ô∏è TIEMPO TOTAL: {total_time:.1f} segundos ({total_time/60:.1f} minutos)")
            
            print(f"\nüéâ ¬°ENTRENAMIENTO COMPLETADO EXITOSAMENTE!")
            print(f"‚úÖ {successful_symbols}/{total_symbols} s√≠mbolos procesados correctamente")
            
            return results
        else:
            print("‚ùå Error en el entrenamiento")
            return None
            
    except Exception as e:
        print(f"‚ùå Error cr√≠tico en ejecuci√≥n autom√°tica: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_quick_test():
    """Ejecutar prueba r√°pida del sistema"""
    
    print("üß™ PRUEBA R√ÅPIDA DEL SISTEMA")
    print("=" * 50)
    
    try:
        # Crear instancia
        ai_system = UltraForexAI_V2()
        
        # Probar con un solo par
        test_symbol = 'EURUSD=X'
        test_style = 'day_trading'
        
        print(f"üéØ Probando: {test_symbol} - {test_style}")
        
        # Obtener datos
        data = ai_system.get_enhanced_data_multi(test_symbol, test_style)
        
        if data is not None and len(data) > 50:
            print(f"‚úÖ Datos obtenidos: {len(data)} registros")
            print(f"üìÖ Rango: {data['Date'].min().date()} to {data['Date'].max().date()}")
            print(f"üìä Features: {len(data.columns)}")
            print(f"üéØ Target: {'Direction' in data.columns}")
            
            # Entrenar ensemble b√°sico
            style_features = ai_system.get_style_features(test_style)
            available_features = [f for f in style_features if f in data.columns]
            
            X = data[available_features].fillna(0).values
            y = data['Direction'].fillna(1).values
            
            print(f"üìä Features disponibles: {len(available_features)}/{len(style_features)}")
            
            # Crear ensemble
            ensemble_result = ai_system.create_ensemble_model_for_style(X, y, test_style)
            
            if ensemble_result:
                print(f"‚úÖ Ensemble creado exitosamente")
                print(f"   üìä Precisi√≥n: {ensemble_result['accuracy']:.3f}")
                print(f"   üéØ Precisi√≥n se√±ales: {ensemble_result['signal_accuracy']:.3f}")
                print(f"   üî¢ Modelos: {ensemble_result['models_count']}")
                
                # Probar concept drift
                print(f"\nüîç Probando concept drift...")
                ensemble = ensemble_result['ensemble']
                
                # Dividir datos para simular drift
                split_point = int(0.7 * len(data))
                historical_data = data.iloc[:split_point]
                recent_data = data.iloc[split_point:]
                
                drift_info = ai_system.detect_concept_drift(ensemble, recent_data, historical_data)
                
                print(f"   üö® Drift detectado: {'S√ç' if drift_info['drift_detected'] else 'NO'}")
                print(f"   üìä Drift score: {drift_info['drift_score']:.3f}")
                
                return ensemble_result
            else:
                print("‚ùå Error creando ensemble")
                return None
        else:
            print("‚ùå Datos insuficientes para prueba")
            return None
            
    except Exception as e:
        print(f"‚ùå Error en prueba r√°pida: {e}")
        return None

# ===== EJECUCI√ìN AUTOM√ÅTICA CUANDO SE EJECUTA EL ARCHIVO =====

if __name__ == "__main__":
    print("üöÄ ULTRAFOREXAI V2 - EJECUCI√ìN AUTOM√ÅTICA")
    print("=" * 80)
    print("üéØ Este archivo se puede ejecutar directamente:")
    print("   python Modelo_ultra.py")
    print("=" * 80)
    
    # Preguntar al usuario qu√© quiere hacer
    import sys
    
    # Inicializar variables
    choice = "1"  # Default a entrenamiento completo
    mode = None
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
    else:
        print("\nüéØ ¬øQu√© quieres hacer?")
        print("1. Entrenamiento completo (20-30 minutos)")
        print("2. Prueba r√°pida (5-10 minutos)")
        print("3. Solo probar integraci√≥n")
        print("4. Probar guardado/carga de modelos")
        print("5. Probar disponibilidad de datos")
        print("6. Probar stacking ensemble")
        
        try:
            choice = input("Selecciona una opci√≥n (1-3): ").strip()
        except:
            choice = "1"  # Default a entrenamiento completo
    
    # Determinar qu√© ejecutar
    if mode == "full" or choice == "1":
        print("\nüöÄ INICIANDO ENTRENAMIENTO COMPLETO...")
        results = run_automatic_training()
        
    elif mode == "quick" or choice == "2":
        print("\nüß™ INICIANDO PRUEBA R√ÅPIDA...")
        result = run_quick_test()
        
    elif mode == "test" or choice == "3":
        print("\nüß™ PROBANDO INTEGRACI√ìN...")
        test_integration()
        
    elif mode == "save_load" or choice == "4":
        print("\nüß™ PROBANDO GUARDADO/CARGA DE MODELOS...")
        test_model_save_load()
        
    elif mode == "data_test" or choice == "5":
        print("\nüß™ PROBANDO DISPONIBILIDAD DE DATOS...")
        test_data_availability()
        
    elif mode == "stacking_test" or choice == "6":
        print("\nüß™ PROBANDO STACKING ENSEMBLE...")
        test_stacking_ensemble()
        
    else:
        print("‚ùå Opci√≥n no v√°lida - Ejecutando entrenamiento completo por defecto")
        results = run_automatic_training()
    
    print("\n" + "="*80)
    print("üéâ ¬°EJECUCI√ìN COMPLETADA!")
    print("="*80)
    
else:
    # Si se importa como m√≥dulo
    print("üì¶ Modelo_ultra.py importado como m√≥dulo")
    print("üéØ Funciones disponibles:")
    print("   - UltraForexAI_V2() - Crear instancia del sistema")
    print("   - run_automatic_training() - Entrenamiento completo")
    print("   - run_quick_test() - Prueba r√°pida")
    print("   - test_integration() - Probar integraci√≥n")
    
    # Para Jupyter/Colab, ejecutar autom√°ticamente
    try:
        import google.colab
        print("\nüöÄ DETECTADO GOOGLE COLAB - EJECUTANDO AUTOM√ÅTICAMENTE...")
        results = run_automatic_training()
    except ImportError:
        try:
            import IPython
            print("\nüìì DETECTADO JUPYTER - EJECUTANDO AUTOM√ÅTICAMENTE...")
            results = run_automatic_training()
        except ImportError:
            print("\nüíª ENTORNO LOCAL - Usar funciones manualmente")

# ===== FUNCI√ìN DE PRUEBA PARA TARGET CREATION MEJORADO =====

def test_improved_target_creation():
    """Probar las mejoras del target creation"""
    
    print("\nüß™ PROBANDO TARGET CREATION MEJORADO")
    print("=" * 60)
    
    try:
        # Crear instancia del sistema
        ai_system = UltraForexAI_V2()
        
        # Crear datos de prueba realistas
        dates = pd.date_range('2024-01-01', periods=200, freq='D')
        np.random.seed(42)  # Para reproducibilidad
        
        # Simular datos de mercado realistas
        base_price = 1.1000  # EURUSD
        returns = np.random.normal(0, 0.01, 200)  # 1% volatilidad diaria
        prices = [base_price]
        
        for ret in returns[1:]:
            new_price = prices[-1] * (1 + ret)
            prices.append(new_price)
        
        test_data = pd.DataFrame({
            'Date': dates,
            'Open': prices,
            'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
            'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
            'Close': prices,
            'Volume': np.random.randint(1000, 10000, 200)
        })
        
        # Agregar features b√°sicos
        test_data['Hour'] = np.random.randint(0, 24, 200)
        test_data['DayOfWeek'] = np.random.randint(0, 7, 200)
        test_data['Month'] = np.random.randint(1, 13, 200)
        
        print(f"üìä Datos de prueba creados: {len(test_data)} registros")
        print(f"üí∞ Rango de precios: {test_data['Close'].min():.4f} - {test_data['Close'].max():.4f}")
        
        # Probar cada estilo de trading
        styles = ['scalping', 'day_trading', 'swing_trading', 'position_trading']
        
        results = {}
        
        for style in styles:
            print(f"\nüéØ Probando {style}...")
            
            try:
                # Crear target mejorado
                target = ai_system.create_style_target(test_data, style)
                
                if target is not None and len(target) == len(test_data):
                    unique_classes = np.unique(target)
                    class_counts = np.bincount(target)
                    class_distribution = dict(zip(unique_classes, class_counts))
                    
                    print(f"‚úÖ {style}: {len(unique_classes)} clases")
                    print(f"üìä Distribuci√≥n: {class_distribution}")
                    
                    # Verificar que hay m√∫ltiples clases
                    if len(unique_classes) >= 2:
                        print(f"‚úÖ {style}: Target v√°lido con m√∫ltiples clases")
                        results[style] = {
                            'success': True,
                            'classes': len(unique_classes),
                            'distribution': class_distribution
                        }
                    else:
                        print(f"‚ùå {style}: Target con una sola clase")
                        results[style] = {
                            'success': False,
                            'classes': len(unique_classes),
                            'distribution': class_distribution
                        }
                else:
                    print(f"‚ùå {style}: Error en target creation")
                    results[style] = {'success': False, 'error': 'Target creation failed'}
                    
            except Exception as e:
                print(f"‚ùå {style}: Error - {e}")
                results[style] = {'success': False, 'error': str(e)}
        
        # Resumen final
        print(f"\n{'='*60}")
        print("üìä RESUMEN DE PRUEBAS")
        print(f"{'='*60}")
        
        successful_styles = sum(1 for r in results.values() if r.get('success', False))
        
        print(f"‚úÖ Estilos exitosos: {successful_styles}/{len(styles)}")
        
        for style, result in results.items():
            status = "‚úÖ" if result.get('success', False) else "‚ùå"
            classes = result.get('classes', 0)
            print(f"  {status} {style}: {classes} clases")
            
            if result.get('success', False):
                distribution = result.get('distribution', {})
                print(f"     üìä Distribuci√≥n: {distribution}")
        
        # Verificar mejoras espec√≠ficas
        print(f"\nüîß VERIFICACI√ìN DE MEJORAS:")
        
        # 1. Verificar que no hay targets con una sola clase
        single_class_targets = sum(1 for r in results.values() 
                                 if r.get('success', False) and r.get('classes', 0) < 2)
        print(f"  {'‚úÖ' if single_class_targets == 0 else '‚ùå'} Targets con una sola clase: {single_class_targets}")
        
        # 2. Verificar distribuci√≥n balanceada
        balanced_targets = 0
        for result in results.values():
            if result.get('success', False):
                distribution = result.get('distribution', {})
                if len(distribution) >= 3:
                    # Verificar que no hay desbalance extremo (>80% en una clase)
                    max_ratio = max(distribution.values()) / sum(distribution.values())
                    if max_ratio < 0.8:
                        balanced_targets += 1
        
        print(f"  {'‚úÖ' if balanced_targets >= 3 else '‚ùå'} Targets balanceados: {balanced_targets}/{len(styles)}")
        
        # 3. Verificar que todos los estilos funcionan
        all_working = all(r.get('success', False) for r in results.values())
        print(f"  {'‚úÖ' if all_working else '‚ùå'} Todos los estilos funcionando: {all_working}")
        
        print(f"\nüéâ {'EXITO' if all_working and single_class_targets == 0 else 'MEJORAS NECESARIAS'}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Error en prueba de target creation: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_target_creation_with_real_data():
    """Probar target creation con datos reales de Yahoo Finance"""
    
    print("\nüß™ PROBANDO TARGET CREATION CON DATOS REALES")
    print("=" * 60)
    
    try:
        # Crear instancia del sistema
        ai_system = UltraForexAI_V2()
        
        # Probar con un s√≠mbolo real
        test_symbol = 'EURUSD=X'
        test_style = 'day_trading'
        
        print(f"üìä Obteniendo datos reales: {test_symbol}")
        
        # Obtener datos reales
        data = ai_system.get_enhanced_data_multi(test_symbol, test_style)
        
        if data is not None and len(data) > 50:
            print(f"‚úÖ Datos obtenidos: {len(data)} registros")
            print(f"üìÖ Rango: {data['Date'].min().date()} to {data['Date'].max().date()}")
            
            # Probar target creation
            target = ai_system.create_style_target(data, test_style)
            
            if target is not None and len(target) == len(data):
                unique_classes = np.unique(target)
                class_counts = np.bincount(target)
                class_distribution = dict(zip(unique_classes, class_counts))
                
                print(f"‚úÖ Target creado exitosamente:")
                print(f"   üìä Clases: {len(unique_classes)}")
                print(f"   üìà Distribuci√≥n: {class_distribution}")
                
                # Verificar calidad del target
                if len(unique_classes) >= 2:
                    print(f"‚úÖ Target v√°lido con m√∫ltiples clases")
                    
                    # Verificar balance
                    max_ratio = max(class_counts) / len(target)
                    if max_ratio < 0.8:
                        print(f"‚úÖ Distribuci√≥n balanceada (max ratio: {max_ratio:.1%})")
                    else:
                        print(f"‚ö†Ô∏è Distribuci√≥n desbalanceada (max ratio: {max_ratio:.1%})")
                    
                    return True
                else:
                    print(f"‚ùå Target con una sola clase: {unique_classes}")
                    return False
            else:
                print(f"‚ùå Error en target creation")
                return False
        else:
            print(f"‚ùå No se pudieron obtener datos reales")
            return False
            
    except Exception as e:
        print(f"‚ùå Error en prueba con datos reales: {e}")
        return False

def test_concept_drift_detection():
    """Probar detecci√≥n de concept drift"""
    
    print("\nüß™ PROBANDO DETECCI√ìN DE CONCEPT DRIFT")
    print("=" * 60)
    
    try:
        # Crear instancia del sistema
        ai_system = UltraForexAI_V2()
        
        # Obtener datos para testing
        test_symbol = 'EURUSD=X'
        test_style = 'day_trading'
        
        print(f"üìä Obteniendo datos para testing: {test_symbol}")
        
        data = ai_system.get_enhanced_data_multi(test_symbol, test_style)
        
        if data is not None and len(data) > 100:
            print(f"‚úÖ Datos obtenidos: {len(data)} registros")
            
            # Crear un modelo simple para testing
            style_features = ai_system.get_style_features(test_style)
            available_features = [f for f in style_features if f in data.columns]
            
            X = data[available_features].fillna(0).values
            y = data['Direction'].fillna(1).values
            
            # Crear ensemble b√°sico
            ensemble_result = ai_system.create_ensemble_model_for_style(X, y, test_style)
            
            if ensemble_result:
                ensemble = ensemble_result['ensemble']
                
                # Dividir datos para simular hist√≥rico vs reciente
                split_point = int(0.7 * len(data))
                historical_data = data.iloc[:split_point]
                recent_data = data.iloc[split_point:]
                
                print(f"üìä Datos hist√≥ricos: {len(historical_data)} registros")
                print(f"üìä Datos recientes: {len(recent_data)} registros")
                
                # Probar detecci√≥n de drift
                drift_info = ai_system.detect_concept_drift(ensemble, recent_data, historical_data)
                
                print(f"\nüìä RESULTADOS DE CONCEPT DRIFT:")
                print(f"   üö® Drift detectado: {'S√ç' if drift_info['drift_detected'] else 'NO'}")
                print(f"   üìà Rendimiento hist√≥rico: {drift_info['historical_performance']:.3f}")
                print(f"   üìâ Rendimiento reciente: {drift_info['recent_performance']:.3f}")
                print(f"   üîç Drift score: {drift_info['drift_score']:.3f}")
                print(f"   üéØ Threshold: {drift_info['threshold']:.3f}")
                
                if drift_info['drift_detected']:
                    print(f"\nüîÑ Probando re-entrenamiento adaptativo...")
                    
                    # Probar re-entrenamiento adaptativo
                    new_model = ai_system.adaptive_model_retraining(test_symbol, test_style, drift_info)
                    
                    if new_model:
                        print(f"‚úÖ Re-entrenamiento exitoso: {new_model['accuracy']:.3f}")
                    else:
                        print(f"‚ùå Error en re-entrenamiento")
                
                return drift_info
            else:
                print(f"‚ùå Error creando ensemble para testing")
                return None
        else:
            print(f"‚ùå Datos insuficientes para testing")
            return None
            
    except Exception as e:
        print(f"‚ùå Error en prueba de concept drift: {e}")
        import traceback
        traceback.print_exc()
        return None

print("‚úÖ Funciones de prueba agregadas:")
print("   - test_improved_target_creation() - Prueba con datos simulados")
print("   - test_target_creation_with_real_data() - Prueba con datos reales")
print("   - test_concept_drift_detection() - Prueba detecci√≥n de concept drift")

print("\nüß™ EJEMPLO DE USO:")
print("""
# Probar target creation mejorado
test_improved_target_creation()

# Probar con datos reales
test_target_creation_with_real_data()

# Probar detecci√≥n de concept drift
test_concept_drift_detection()
""")

print("\n" + "="*80)

# Move these functions inside the class definition

def test_model_save_load():
    """Probar el guardado y carga de modelos"""
    
    print("\nüß™ PROBANDO GUARDADO Y CARGA DE MODELOS")
    print("=" * 60)
    
    try:
        # Crear instancia del sistema
        ai_system = UltraForexAI_V2()
        
        # Verificar estructura
        ai_system.verify_model_structure()
        ai_system.clean_old_models()
        
        # Probar con un s√≠mbolo y estilo
        test_symbol = 'EURUSD=X'
        test_style = 'day_trading'
        
        print(f"üéØ Probando: {test_symbol} - {test_style}")
        
        # Obtener datos
        data = ai_system.get_enhanced_data_multi(test_symbol, test_style)
        
        if data is not None and len(data) > 50:
            print(f"‚úÖ Datos obtenidos: {len(data)} registros")
            
            # Preparar features
            style_features = ai_system.get_style_features(test_style)
            available_features = [f for f in style_features if f in data.columns]
            
            X = data[available_features].fillna(0).values
            y = data['Direction'].fillna(1).values
            
            print(f"üìä Features disponibles: {len(available_features)}/{len(style_features)}")
            
            # Crear ensemble
            ensemble_result = ai_system.create_ensemble_model_for_style(X, y, test_style)
            
            if ensemble_result:
                print(f"‚úÖ Ensemble creado exitosamente")
                print(f"   üìä Precisi√≥n: {ensemble_result['accuracy']:.3f}")
                print(f"   üéØ Precisi√≥n se√±ales: {ensemble_result['signal_accuracy']:.3f}")
                print(f"   üî¢ Modelos: {ensemble_result['models_count']}")
                
                # Guardar modelo
                print(f"\nüíæ Guardando modelo...")
                save_result = ai_system.save_ensemble_models(test_symbol, {test_style: ensemble_result})
                
                if save_result and test_style in save_result:
                    print(f"‚úÖ Modelo guardado: {save_result[test_style]['local_path']}")
                    
                    # Intentar cargar modelo
                    print(f"\nüìÇ Cargando modelo...")
                    current_data = data.tail(1)  # √öltimo registro para predicci√≥n
                    
                    prediction = ai_system.get_ensemble_prediction(test_symbol, test_style, current_data)
                    
                    if prediction:
                        print(f"‚úÖ Modelo cargado exitosamente")
                        print(f"   üéØ Se√±al: {prediction['signal']}")
                        print(f"   üìä Confianza: {prediction['confidence']:.3f}")
                        print(f"   üìà Probabilidades: {prediction['probabilities']}")
                        print(f"   üî¢ Modelos: {prediction['model_info']['models_count']}")
                        
                        return True
                    else:
                        print(f"‚ùå Error cargando modelo")
                        return False
                else:
                    print(f"‚ùå Error guardando modelo")
                    return False
            else:
                print(f"‚ùå Error creando ensemble")
                return False
        else:
            print(f"‚ùå Datos insuficientes para prueba")
            return False
            
    except Exception as e:
        print(f"‚ùå Error en prueba de guardado/carga: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_availability():
    """Test para verificar disponibilidad de datos para scalping"""
    
    logger.info("üß™ Probando disponibilidad de datos para scalping...")
    
    # Crear instancia del sistema
    ai_system = UltraForexAI_V2()
    
    # Test con EURUSD
    symbol = 'EURUSD=X'
    trading_style = 'scalping'
    
    try:
        # Obtener datos
        logger.info(f"üìä Obteniendo datos para {symbol} ({trading_style})...")
        data = ai_system.get_enhanced_data_multi(symbol, trading_style)
        
        if data is None:
            logger.error("‚ùå No se pudieron obtener datos")
            return False
        
        logger.info(f"‚úÖ Datos obtenidos: {len(data)} registros")
        
        # Validar calidad
        is_valid, issues = ai_system.validate_data_quality(data, trading_style)
        
        if is_valid:
            logger.info("‚úÖ Datos v√°lidos para scalping")
            logger.info(f"   - Registros: {len(data)}")
            logger.info(f"   - Features: {len(data.columns)}")
            logger.info(f"   - Rango temporal: {data.index[0]} a {data.index[-1]}")
            
            # Verificar target
            target = ai_system.create_style_target(data, trading_style)
            unique_classes = np.unique(target)
            logger.info(f"   - Target clases: {unique_classes}")
            logger.info(f"   - Target distribuci√≥n: {np.bincount(target)}")
            
            return True
        else:
            logger.warning("‚ö†Ô∏è Problemas de calidad detectados:")
            for issue in issues:
                logger.warning(f"   - {issue}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error en test de datos: {e}")
        return False

# Agregar funci√≥n de test
UltraForexAI_V2.test_data_availability = test_data_availability

def test_stacking_ensemble():
    """Probar el stacking ensemble"""
    
    print("\nüß™ PROBANDO STACKING ENSEMBLE")
    print("=" * 60)
    
    try:
        # Crear instancia del sistema
        ai_system = UltraForexAI_V2()
        
        # Verificar estructura
        ai_system.verify_model_structure()
        ai_system.clean_old_models()
        
        # Probar con un s√≠mbolo y estilo
        test_symbol = 'EURUSD=X'
        test_style = 'day_trading'
        
        print(f"üéØ Probando: {test_symbol} - {test_style}")
        
        # Obtener datos
        data = ai_system.get_enhanced_data_multi(test_symbol, test_style)
        
        if data is not None and len(data) > 50:
            print(f"‚úÖ Datos obtenidos: {len(data)} registros")
            
            # Preparar features
            style_features = ai_system.get_style_features(test_style)
            available_features = [f for f in style_features if f in data.columns]
            
            X = data[available_features].fillna(0).values
            y = data['Direction'].fillna(1).values
            
            print(f"üìä Features disponibles: {len(available_features)}/{len(style_features)}")
            
            # Crear ensemble con stacking
            ensemble_result = ai_system.create_ensemble_model_for_style(X, y, test_style)
            
            if ensemble_result:
                ensemble = ensemble_result['ensemble']
                
                print(f"‚úÖ Stacking ensemble creado exitosamente")
                print(f"   üìä Precisi√≥n: {ensemble_result['accuracy']:.3f}")
                print(f"   üéØ Precisi√≥n se√±ales: {ensemble_result['signal_accuracy']:.3f}")
                print(f"   üî¢ Modelos base: {ensemble_result['models_count']}")
                print(f"   üß† Meta-modelo: {ensemble_result.get('meta_model_type', 'RandomForest')}")
                
                # Probar predicci√≥n
                print(f"\nüéØ Probando predicci√≥n...")
                current_data = data.tail(1)  # √öltimo registro para predicci√≥n
                
                prediction = ensemble.predict(X[-1:])
                prediction_proba = ensemble.predict_proba(X[-1:])
                
                signal_map = {0: 'SELL', 1: 'HOLD', 2: 'BUY'}
                signal = signal_map.get(prediction[0], 'HOLD')
                confidence = float(np.max(prediction_proba[0]))
                
                print(f"   üéØ Se√±al: {signal}")
                print(f"   üìä Confianza: {confidence:.3f}")
                print(f"   üìà Probabilidades: SELL={prediction_proba[0][0]:.3f}, HOLD={prediction_proba[0][1]:.3f}, BUY={prediction_proba[0][2]:.3f}")
                
                # Probar importancia de modelos
                print(f"\nüìä Importancia de modelos base:")
                importance = ensemble.get_model_importance()
                model_names = list(ensemble.base_models.keys())
                
                for i, model_name in enumerate(model_names):
                    if i < len(importance):
                        print(f"   {model_name}: {importance[i]:.3f}")
                
                # Probar actualizaci√≥n del meta-modelo
                print(f"\nüîÑ Probando actualizaci√≥n del meta-modelo...")
                X_val = X[-10:]  # √öltimos 10 registros como validaci√≥n
                y_val = y[-10:]
                
                ensemble.update_meta_model(X_val, y_val)
                print(f"   ‚úÖ Meta-modelo actualizado")
                
                return ensemble_result
            else:
                print(f"‚ùå Error creando stacking ensemble")
                return None
        else:
            print(f"‚ùå Datos insuficientes para prueba")
            return None
            
    except Exception as e:
        print(f"‚ùå Error en prueba de stacking: {e}")
        import traceback
        traceback.print_exc()
        return None
