# optimization_example.py - Ejemplo pr√°ctico de optimizaci√≥n de hiperpar√°metros
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import yfinance as yf
from hyperparameter_optimization import AdvancedHyperparameterOptimizer, integrate_hyperparameter_optimization
from ai_models import AdvancedTradingAI
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def demonstrate_hyperparameter_optimization():
    """Demuestra el uso del sistema de optimizaci√≥n de hiperpar√°metros"""
    
    print("üöÄ DEMOSTRACI√ìN: Optimizaci√≥n Avanzada de Hiperpar√°metros")
    print("=" * 60)
    
    # 1. Obtener datos de mercado
    print("\nüìä 1. Obteniendo datos de mercado...")
    symbol = "AAPL"
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)  # 1 a√±o de datos
    
    try:
        ticker = yf.Ticker(symbol)
        data = ticker.history(start=start_date, end=end_date, interval="1d")
        print(f"‚úÖ Datos obtenidos: {len(data)} d√≠as de {symbol}")
    except Exception as e:
        print(f"‚ùå Error obteniendo datos: {e}")
        return
    
    # 2. Preparar datos para optimizaci√≥n
    print("\nüîß 2. Preparando datos para optimizaci√≥n...")
    
    # Crear modelo AI b√°sico
    ai_model = AdvancedTradingAI()
    
    # Crear features
    data_with_features = ai_model.create_features(data)
    
    # Preparar features para optimizaci√≥n
    feature_columns = [
        'rsi', 'macd', 'momentum_5', 'momentum_10', 'momentum_20',
        'volatility_5', 'volatility_10', 'volume_ratio', 'high_low_ratio',
        'trend_5_20', 'trend_20_50', 'day_of_week', 'month'
    ]
    
    X = data_with_features[feature_columns].fillna(0)
    y = ai_model.create_signals(data_with_features)
    
    # Filtrar datos v√°lidos
    valid_mask = ~X.isin([np.inf, -np.inf]).any(axis=1)
    X = X[valid_mask]
    y = y[valid_mask]
    
    print(f"‚úÖ Datos preparados: {len(X)} muestras, {len(feature_columns)} features")
    
    # 3. Crear optimizador
    print("\n‚öôÔ∏è 3. Configurando optimizador...")
    optimizer = AdvancedHyperparameterOptimizer(
        data_source=None,
        max_trials=20,  # Reducido para demostraci√≥n
        cv_folds=3      # Reducido para demostraci√≥n
    )
    
    # 4. Optimizar modelos individuales
    print("\nüîç 4. Optimizando modelos individuales...")
    
    # Random Forest
    print("   üîß Optimizando Random Forest...")
    try:
        rf_params = optimizer.optimize_random_forest(X, y)
        print(f"   ‚úÖ Random Forest optimizado:")
        for param, value in rf_params.items():
            print(f"      {param}: {value}")
    except Exception as e:
        print(f"   ‚ùå Error optimizando Random Forest: {e}")
    
    # XGBoost
    print("   üîß Optimizando XGBoost...")
    try:
        xgb_params = optimizer.optimize_xgboost(X, y)
        print(f"   ‚úÖ XGBoost optimizado:")
        for param, value in xgb_params.items():
            print(f"      {param}: {value}")
    except Exception as e:
        print(f"   ‚ùå Error optimizando XGBoost: {e}")
    
    # LightGBM
    print("   üîß Optimizando LightGBM...")
    try:
        lgb_params = optimizer.optimize_lightgbm(X, y)
        print(f"   ‚úÖ LightGBM optimizado:")
        for param, value in lgb_params.items():
            print(f"      {param}: {value}")
    except Exception as e:
        print(f"   ‚ùå Error optimizando LightGBM: {e}")
    
    # 5. Optimizar ensemble
    print("\nüéØ 5. Optimizando ensemble...")
    try:
        ensemble_params = optimizer.optimize_ensemble(X, y)
        print(f"   ‚úÖ Ensemble optimizado:")
        for param, value in ensemble_params.items():
            print(f"      {param}: {value}")
    except Exception as e:
        print(f"   ‚ùå Error optimizando ensemble: {e}")
    
    # 6. Crear modelos optimizados
    print("\nüèóÔ∏è 6. Creando modelos optimizados...")
    optimized_models = optimizer.create_optimized_models(X, y)
    
    print(f"   ‚úÖ Modelos creados: {list(optimized_models.keys())}")
    
    # 7. Guardar resultados
    print("\nüíæ 7. Guardando resultados...")
    optimizer.save_optimization_results()
    
    # 8. Comparar rendimiento
    print("\nüìà 8. Comparando rendimiento...")
    compare_model_performance(X, y, optimized_models)
    
    print("\n‚úÖ Optimizaci√≥n completada exitosamente!")
    return optimizer, optimized_models

def compare_model_performance(X: pd.DataFrame, y: pd.Series, optimized_models: dict):
    """Compara el rendimiento de los modelos optimizados"""
    
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    # Dividir datos
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    results = {}
    
    for model_name, model in optimized_models.items():
        if model_name == 'ensemble':
            continue  # Ensemble se maneja por separado
        
        try:
            # Entrenar modelo
            model.fit(X_train, y_train)
            
            # Predecir
            y_pred = model.predict(X_test)
            
            # Calcular m√©tricas
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            
            results[model_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            
            print(f"   üìä {model_name.upper()}:")
            print(f"      Accuracy: {accuracy:.3f}")
            print(f"      Precision: {precision:.3f}")
            print(f"      Recall: {recall:.3f}")
            print(f"      F1-Score: {f1:.3f}")
            
        except Exception as e:
            print(f"   ‚ùå Error evaluando {model_name}: {e}")
    
    # Evaluar ensemble si existe
    if 'ensemble' in optimized_models:
        try:
            ensemble = optimized_models['ensemble']
            weights = ensemble['weights']
            models = ensemble['models']
            
            # Predicciones ponderadas
            predictions = []
            for i, model in enumerate(models):
                if model is not None:
                    pred_proba = model.predict_proba(X_test)[:, 1]
                    weight = list(weights.values())[i]
                    predictions.append(pred_proba * weight)
            
            if predictions:
                ensemble_pred = np.sum(predictions, axis=0)
                y_pred_ensemble = (ensemble_pred > 0.5).astype(int)
                
                accuracy = accuracy_score(y_test, y_pred_ensemble)
                precision = precision_score(y_test, y_pred_ensemble, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred_ensemble, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred_ensemble, average='weighted', zero_division=0)
                
                print(f"   üìä ENSEMBLE:")
                print(f"      Accuracy: {accuracy:.3f}")
                print(f"      Precision: {precision:.3f}")
                print(f"      Recall: {recall:.3f}")
                print(f"      F1-Score: {f1:.3f}")
                
        except Exception as e:
            print(f"   ‚ùå Error evaluando ensemble: {e}")

def show_optimization_benefits():
    """Muestra los beneficios de la optimizaci√≥n de hiperpar√°metros"""
    
    print("\nüéØ BENEFICIOS DE LA OPTIMIZACI√ìN AVANZADA")
    print("=" * 50)
    
    benefits = [
        {
            "title": "üéØ Precisi√≥n Mejorada",
            "description": "Los modelos optimizados pueden mejorar la precisi√≥n en 10-30%",
            "example": "Random Forest: 65% ‚Üí 78% accuracy"
        },
        {
            "title": "‚ö° Velocidad de Entrenamiento",
            "description": "Hiperpar√°metros √≥ptimos reducen el tiempo de entrenamiento",
            "example": "XGBoost: 2 horas ‚Üí 45 minutos"
        },
        {
            "title": "üõ°Ô∏è Robustez",
            "description": "Modelos m√°s estables en diferentes condiciones de mercado",
            "example": "Mejor rendimiento en mercados vol√°tiles"
        },
        {
            "title": "üìä Validaci√≥n Temporal",
            "description": "TimeSeriesSplit previene overfitting en datos temporales",
            "example": "Simula condiciones reales de trading"
        },
        {
            "title": "üîß Automatizaci√≥n",
            "description": "Optimizaci√≥n autom√°tica sin intervenci√≥n manual",
            "example": "Auto-optimizaci√≥n cada 6 horas"
        }
    ]
    
    for benefit in benefits:
        print(f"\n{benefit['title']}")
        print(f"   {benefit['description']}")
        print(f"   üìà {benefit['example']}")

def explain_hyperparameters():
    """Explica los hiperpar√°metros m√°s importantes"""
    
    print("\nüìö EXPLICACI√ìN DE HIPERPAR√ÅMETROS")
    print("=" * 40)
    
    hyperparams = {
        "Random Forest": {
            "n_estimators": "N√∫mero de √°rboles (m√°s = mejor precisi√≥n, pero m√°s lento)",
            "max_depth": "Profundidad m√°xima de cada √°rbol (evita overfitting)",
            "min_samples_split": "M√≠nimo de muestras para dividir un nodo",
            "min_samples_leaf": "M√≠nimo de muestras en hojas (evita overfitting)"
        },
        "XGBoost": {
            "learning_rate": "Tasa de aprendizaje (m√°s bajo = m√°s preciso pero m√°s lento)",
            "max_depth": "Profundidad m√°xima de √°rboles",
            "subsample": "Fracci√≥n de muestras para cada √°rbol",
            "colsample_bytree": "Fracci√≥n de features para cada √°rbol"
        },
        "LightGBM": {
            "num_leaves": "N√∫mero de hojas (m√°s = m√°s complejo)",
            "learning_rate": "Tasa de aprendizaje",
            "min_child_samples": "M√≠nimo de muestras en hojas",
            "reg_alpha": "Regularizaci√≥n L1",
            "reg_lambda": "Regularizaci√≥n L2"
        }
    }
    
    for model, params in hyperparams.items():
        print(f"\nüîß {model}:")
        for param, description in params.items():
            print(f"   ‚Ä¢ {param}: {description}")

if __name__ == "__main__":
    # Mostrar explicaciones
    show_optimization_benefits()
    explain_hyperparameters()
    
    # Ejecutar demostraci√≥n
    print("\n" + "="*60)
    print("üöÄ INICIANDO DEMOSTRACI√ìN PR√ÅCTICA")
    print("="*60)
    
    try:
        optimizer, models = demonstrate_hyperparameter_optimization()
        print("\n‚úÖ Demostraci√≥n completada exitosamente!")
    except Exception as e:
        print(f"\n‚ùå Error en demostraci√≥n: {e}")
        print("üí° Aseg√∫rate de tener instaladas las dependencias:")
        print("   pip install optuna xgboost lightgbm scikit-learn") 